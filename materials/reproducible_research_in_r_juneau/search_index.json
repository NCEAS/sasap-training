[
["index.html", "Reproducible Analysis With R 1 Introduction 1.1 The Reproducibility Crisis 1.2 What is needed for computational reproducibility? 1.3 Conceptualizing workflows 1.4 Course Objectives 1.5 Course details", " Reproducible Analysis With R November 28-29, 2017 1 Introduction Reproducibility is the hallmark of science, which is based on empirical observations coupled with explanatory models. While reproducibility encompasses the full science lifecycle, and includes issues such as methodological consistency and treatment of bias, in this course we will focus on computational reproducibility: the ability to document data, analyses, and models sufficiently for other researchers to be able to understand and ideally re-execute the computations that led to scientific results and conclusions. 1.1 The Reproducibility Crisis Ioannidis (2005) highlighted a growing crisis in reproducibility of science when he wrote that “Most Research Findings Are False for Most Research Designs and for Most Fields”. Ioannidis outlined ways in which the research process has lead to inflated effect sizes and hypothesis tests that codify existing biases. Subsequent research has confirmed that reproducibility is low across many fields, including genetics (Ioannidis et al. 2009) and psychology (Open Science Collaboration 2015), among others. For example, effect size has been shown to significantly decrease in repeated experiments in psychology (1.1). Figure 1.1: Effect size decreases in replicated experiments (Open Science Collaboration 2015). 1.2 What is needed for computational reproducibility? The first step towards addressing these issues is to be able to evaluate the data, analyses, and models on which conclusions are drawn. Under current practice, this can be difficult because data are typically unavailable, the method sections of papers do not detail the computational approaches used, and analyses and models are often conducted in graphical programs, or, when scripted analyses are employed, the code is not available. And yet, this is easily remedied. Researchers can achieve computational reproducibility through open science approaches, including straightforward steps for archiving data and code openly along with the scientific workflows describing the provenance of scientific results (e.g., Hampton et al. (2015), Munafò et al. (2017)). 1.3 Conceptualizing workflows Scientific workflows encapsulate all of the steps from data acquisition, cleaning, transformation, integration, analysis, and visualization. Figure 1.2: Scientific workflows and provenance capture the multiple steps needed to reproduce a scientific result from raw data. Workflows can range in detail from simple flowcharts (1.2) to fully executable scripts. R scripts and python scripts are a textual form of a workflow, and when researchers publish specific versions of the scripts and data used in an analysis, it becomes far easier to repeat their computations and understand the provenance of their conclusions. Within many science disciplines, researchers are seeing the power of building reproducible workflows. For example, within fisheries science, the ICES Transparent Assessment Framework (TAF) is being developed to help researchers share stock assessments consistently and completely. 1.4 Course Objectives This event will cover techniques for building reproducible analysis workflows using the R programming language through a series of hands-on coding sessions. We will use examples from integrating salmon brood data across the state of Alaska to show how heterogeneous data can be cleaned, integrated, and documented through workflows written in RMarkdown. After an overview of the use of RMarkdown for literate analysis, we will dive into critical topics in data science, including version control, data modeling, cleaning, and integration, and then data visualization both for publications and the web. Major topics will include: Literate analysis using RMarkdown Version control for managing scientific code Data modeling, cleaning, and integration Publishing data and code Data visualization for the web 1.5 Course details This mini-course will be a hands-on experience, with most of the material presented through a series of tutorials. It is meant as a survey of techniques that will motivate participants to continue self-study in reproducible science. While prior knowledge of R is a pre-requisite for the mini-course, participants enter with a range of experience with scientific computing in R. We will try to accommodate this diversity, while maintaining a reasonable pace through the materials. We will conduct the course in a supportive and friendly manner, working to build skills, confidence, and curiosity about data science. We encourage an informal atmosphere, where questions and discussion are welcome. References "],
["example-brood-table-analysis.html", "2 Example: Brood Table Analysis 2.1 Introduction 2.2 Datasets 2.3 Reformatting 2.4 Merging 2.5 Quality Assurance 2.6 Analysis", " 2 Example: Brood Table Analysis 2.1 Introduction Brood tables, also called run reconstructions, utilize annual estimates of the total run (commercial catch plus escapement), and samples of ages, to estimate the number of recruits per age class. These data are useful for salmon biologists to understand salmon productivity and salmon life histories. These data can come in a number of different formats, but generally follow the pattern of: 1. Rows for each brood year 2. Columns for the estimated number of fish in each age class. See the example below, showing a Sockeye salmon brood table from the Goodnews River, Alaska: Sometimes other columns are included, such as return years in this example (again for Sockeye salmon) from Coghill Lake, Alaska: If you are interested in analysing trends across multiple stocks, the many different ways brood tables can be presented make integrating multiple brood tables a big challenge. In this exercise we will intoduce some ways to reformat, merge, and reshape a set of brood tables from around the state of Alaska. These datasets were gathered as part of the SASAP working group research from a number of different ADFG offices. We will start with the raw (unmodified) data for each stock, run each stock’s brood table through a separate R script to normalize the format, integrate all of the brood tables together, and finally do an exploratory analysis across stocks. 2.2 Datasets As part of the SASAP project, brood tables for 48 Sockeye salmon stocks were collected. Table 2.1 shows a list of these stocks, along with other regional and location information. These stocks range geographically from Washington to Alaska. Although temporal coverage varies by stock, many of the brood tables were updated in 2016, and some have reconstructions dating back to 1922. Figure 2.1 indicates the approximate location of the salmon stocks in Table 2.1. Figure 2.1: Location of stocks used in this data integration. Salmonid icon by Servien (vectorized by T. Michael Keesey) CC-BY-SA, available at Phylopic 2.3 Reformatting First we need to change the brood table column names so that they are in a consistent format. Here we show an example of how one of our source tables (Coghill Lake, from above) is reformatted. Prior to diving into reformatting, it is good to have an idea of what columns will be necessary in your merged data table. To identify different stocks, certainly a column will be needed for the stock name (usually a river name) and the species. If you wish to analyze across different regions or areas, you may want to have a column for region as well. Of course you will also need columns for brood year and all of the possible age classes you might encounter. A quality flag indicating whether data should be used in analysis or not based on set critera is also useful. Finally, it is also a good idea to have a unique identifier for each brood table that goes into the merged data table so that you can easily add data by stock, or summarize data by stock. With that in mind, we will be reformatting all of the brood tables so they have the following column names: Stock.ID BroodYear Region Sub.Region UseFlag BroodYear TotalEscapement R0.1 ... R4.5 Now let’s read in the brood table. After a quick visual inspection in Excel, we can see that the header doesn’t start until the sixth row, so we should tell read_excel to skip the first six lines: coghill_brood &lt;- read_excel(&#39;data/original/112_COGHILL_BROOD_TABLE.xls&#39;, skip = 6) colnames(coghill_brood) ## [1] &quot;Year&quot; &quot;Escpmt.&quot; &quot;Year__1&quot; &quot;1.1&quot; ## [5] &quot;0.2&quot; &quot;Year__2&quot; &quot;0.3&quot; &quot;Aged 1.2&quot; ## [9] &quot;Aged 2.1&quot; &quot;Year__3&quot; &quot;Aged 1.3&quot; &quot;Aged 2.2&quot; ## [13] &quot;Year__4&quot; &quot;Aged 1.4&quot; &quot;Aged 2.3&quot; &quot;Year__5&quot; ## [17] &quot;Aged 2.4&quot; &quot;Return&quot; &quot;Year__6&quot; &quot;Return__1&quot; ## [21] &quot;Return/spawner&quot; There are definitely some redundant and confusing columns here that we need to remove, and some we need to add according to the list above. First we remove the columns we don’t want - namely all of the return years, the total return, and recruits/spawner. Note that we can always calculate these columns again later from the rest of the data. coghill_brood &lt;- coghill_brood %&gt;% select(-Year__1, -Year__2, -Year__3, -Year__4, -Year__5, -Year__6, -Return, -Return__1, -`Return/spawner`) Now we need to fill in the information that is missing by adding columns: 'Stock.ID', 'Species', 'Stock','Region','Sub.Region','UseFlag' #add stock information columns coghill_brood$Stock.ID &lt;- 139 #preassigned Stock.ID coghill_brood$Species &lt;- &#39;Sockeye&#39; coghill_brood$Stock &lt;- &#39;Coghill&#39; coghill_brood$Region &lt;- &#39;PWS&#39; coghill_brood$Sub.Region &lt;- &#39;PWS&#39; coghill_brood$UseFlag &lt;- 1 Note that since we have no reason to suspect any data are not up to quality standards yet, we fill the UseFlag columns with 1s. The rest of the columns are still not quite in the format that we decided on so we will rename them. names(coghill_brood) &lt;- c(&quot;BroodYear&quot;, &quot;TotalEscapement&quot;, &quot;R1.1&quot;, &quot;R0.2&quot;, &quot;R0.3&quot;, &quot;R1.2&quot;, &quot;R2.1&quot;, &quot;R1.3&quot;, &quot;R2.2&quot;, &quot;R1.4&quot;, &quot;R2.3&quot;, &quot;R2.4&quot;, &quot;Stock.ID&quot;, &quot;Species&quot;, &#39;Stock&#39;, &quot;Region&quot;, &quot;Sub.Region&quot;, &quot;UseFlag&quot;) Finally, you may want to reorder the columns into something more intuitive. coghill_brood &lt;- coghill_brood[,c(&#39;Stock.ID&#39;, &#39;Species&#39;, &#39;Stock&#39;,&#39;Region&#39;,&#39;Sub.Region&#39;, &#39;UseFlag&#39;, &#39;BroodYear&#39;,&#39;TotalEscapement&#39;,&#39;R0.2&#39;, &#39;R0.3&#39;, &#39;R1.1&#39;,&#39;R1.2&#39;,&#39;R1.3&#39;,&#39;R1.4&#39;, &#39;R2.1&#39;,&#39;R2.2&#39;,&#39;R2.3&#39;,&#39;R2.4&#39;)] And very lastly, write the file to a new directory where all of the individually reformatted brood tables will be kept. write.csv(coghill_brood, &#39;data/reformatted/Coghill_sockeye.csv&#39;, row.names = F) From top to bottom, here is what this reformatting script looks like: knitr::read_chunk(&quot;data/data_formatting_scripts/Coghill_formatting.R&quot;) We write a script similar to this for each stock that we would like to integrate into the brood table analysis. Note that many of the original brood tables come in multi-tab spreadsheets containing multiple stocks with each tab corresponding to a unique stock, therefore some of these stock-specific scripts access the same original excel document, but different sheets. In the case where the sheets are all formatted the same (Bristol Bay), the script contains a function which processes each sheet and writes individual .csv files for each stock. Here is a listing of the original data files: ## [1] &quot;112_COGHILL_BROOD_TABLE.xls&quot; ## [2] &quot;2016 Bristol Bay Updated Brood Table 11.4.16.xlsx&quot; ## [3] &quot;Chilkoot Lake brood table 2016.xlsx&quot; ## [4] &quot;Copper_River_Catch_byAge_SOCKEYE.xls.xlsx&quot; ## [5] &quot;Eshamy_Sockeye.csv&quot; ## [6] &quot;Fraser_SX_brood_tables_July2017.xlsx&quot; ## [7] &quot;Kenai and Kasilof Sockeye Salmon Brood Tables.xlsx&quot; ## [8] &quot;Lake WA Brood Table 2015.xlsx&quot; ## [9] &quot;Middle Fork Goodnews River Sockeye Brood Table.xlsx&quot; ## [10] &quot;Redoubt Lake - updated brood table_GR.xlsx&quot; ## [11] &quot;source_information.csv&quot; ## [12] &quot;Speel Lake sockeye run reconstruction 1986-2013_GR.xlsx&quot; ## [13] &quot;StockInfo.csv&quot; ## [14] &quot;TogiakSockeye.xlsx&quot; ## [15] &quot;westward brood tables.xlsx&quot; And of the reformatting scripts: ## [1] &quot;BristolBay_formatting.R&quot; ## [2] &quot;Chilkoot_formatting.R&quot; ## [3] &quot;Coghill_formatting.R&quot; ## [4] &quot;Copper_formatting.R&quot; ## [5] &quot;Eshamy_formatting.R&quot; ## [6] &quot;FraserRiver_formatting.R&quot; ## [7] &quot;Goodnews_formatting.R&quot; ## [8] &quot;KenaiAndKasilof_Sheet1_Kenai_formatting.R&quot; ## [9] &quot;KenaiAndKasilof_Sheet2_Kasilof_formatting.R&quot; ## [10] &quot;Redoubt_formatting.R&quot; ## [11] &quot;Speel_formatting.R&quot; ## [12] &quot;Togiak_formatting.R&quot; ## [13] &quot;Washington_formatting.R&quot; ## [14] &quot;Westward_Sheet1_Nelson_formatting.R&quot; ## [15] &quot;Westward_Sheet10_Ayakulik_formatting.R&quot; ## [16] &quot;Westward_Sheet2_Bear_formatting.R&quot; ## [17] &quot;Westward_Sheet3_ChignikLake_formatting.R&quot; ## [18] &quot;Westward_Sheet4_BlackLake_formatting.R&quot; ## [19] &quot;Westward_Sheet5_UpperStationLate_formatting.R&quot; ## [20] &quot;Westward_Sheet6_UpperStationEarly_formatting.R&quot; ## [21] &quot;Westward_Sheet7_KarlukLate_formatting.R&quot; ## [22] &quot;Westward_Sheet8_KarlukEarly_formatting.R&quot; ## [23] &quot;Westward_Sheet9_Frazer_formatting.R&quot; 2.3.1 Batch Reformatting Once a reformatting script has been written for all of the data files that are going to be incorporated, we can source all of the scripts within the reformatting folder and run them, which will create all of the reformatted data files. formatting_scripts &lt;- dir(&quot;data/data_formatting_scripts/&quot;, full.names = TRUE, pattern = &quot;*\\\\.[rR]&quot;) sapply(formatting_scripts, source) 2.4 Merging Now that all of the data tables have been reformatted, we can read them in and merge them into a single file. We use the function bind_rows here to merge the files together since some of these files have age classes that others don’t. The bind_rows function adds in all columns that exist, filling in the columns that don’t have values for a particular stock with NA. path1 &lt;- &#39;data/reformatted/&#39; files &lt;- dir(path1, include.dirs = FALSE) brood &lt;- do.call(bind_rows, lapply(file.path(path1,files), read.csv, stringsAsFactors = F)) Using bind_rows made the column order a little strange, so we’ll rearrange again. brood &lt;- brood[, c(&#39;Stock.ID&#39;, &#39;Species&#39;, &#39;Stock&#39;,&#39;Region&#39;,&#39;Sub.Region&#39;, &#39;UseFlag&#39;,&#39;BroodYear&#39;, &#39;TotalEscapement&#39;,&#39;R0.1&#39;,&#39;R0.2&#39;, &#39;R0.3&#39;,&#39;R0.4&#39;, &#39;R0.5&#39;, &#39;R1.1&#39;,&#39;R1.2&#39;,&#39;R1.3&#39;,&#39;R1.4&#39;,&#39;R1.5&#39;, &#39;R2.1&#39;,&#39;R2.2&#39;,&#39;R2.3&#39;,&#39;R2.4&#39;, &#39;R3.1&#39;,&#39;R3.2&#39;,&#39;R3.3&#39;,&#39;R3.4&#39;, &#39;R4.1&#39;,&#39;R4.2&#39; ,&#39;R4.3&#39;)] 2.5 Quality Assurance 2.5.1 Acceptable values Now we should do some checks to make sure that the data are up to standards and that there were no errors during the reformatting process. Below is a list of expected values for this dataset that we can check for: Year values are numeric and within reasonable bounds (1900s to present), no NA values Age class columns are numeric with values greater than or equal to zero, NA allowed Age class columns are integers Escapement values are numeric and greater than 0, NA allowed These checks are very simple to make - we will simply use the range function on these three columns. 2.5.1.1 Checking BroodYear range(brood$BroodYear) ## [1] 1922 2016 The year data look good. 2.5.1.2 Checking Age Classes Here we use the select function, combined with matches and a regular expression, to select only the age class columns. range(select(brood, matches(&quot;R[0-9].[0-9]&quot;)), na.rm = T) ## [1] 0 47748183 The data in all of the age class columns look good in terms of range. Now we check to see if the values look like integers. R0.1 R0.2 R0.3 R0.4 R0.5 R1.1 R1.2 R1.3 R1.4 R1.5 R2.1 R2.2 R2.3 R2.4 R3.1 R3.2 R3.3 R3.4 R4.1 R4.2 R4.3 NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA 0 NA NA NA NA NA NA NA NA NA NA NA NA 0 NA NA NA 0 NA NA 0.0000000 0 NA NA NA NA NA NA NA 0 NA NA NA 0 0 NA NA 13499.399 0 NA 0.0000000 0.0000016 0 NA NA NA NA NA NA 0 0 NA NA 13119.73 0 0 NA 20219.16 8577.297 0 0 0.0000002 0.0000000 0 NA NA NA NA NA 0 0 0 NA 331300.0 338122.23 0 0 0.0e+00 286853.66 50738.894 0 0 772.7989217 1312.3211884 0 NA NA NA NA 0 0 0 0 0 121361.3 116448.34 0 0 8.1e-06 166914.94 43428.894 0 0 0.0000000 0.0000000 0 NA NA NA Because much of these data were imported from Excel spreadsheets which use formulas to calculate the values in each cell for the recruits per age class, many of these recruit values contain real numbers as opposed to integers. Since the data were meant to be interpreted as integers (number of fish), we use the trunc function to round all of the values in the age class columns down to the nearest integer. This line runs the trunc function only the columns in the brood data.frame which match the pattern that defines the age class columns brood &lt;- mutate_at(brood, vars(matches(&quot;R[0-9].[0-9]&quot;)), trunc) 2.5.1.3 Checking Escapement range(brood$TotalEscapement, na.rm = T) ## [1] 0 24325926 Some escapement data have a value of 0 - which does not make sense in this context. Let’s examine these 0 values, Stock.ID Species Stock Region Sub.Region UseFlag BroodYear TotalEscapement R0.1 R0.2 R0.3 R0.4 R0.5 R1.1 R1.2 R1.3 R1.4 R1.5 R2.1 R2.2 R2.3 R2.4 R3.1 R3.2 R3.3 R3.4 R4.1 R4.2 R4.3 163 Sockeye Togiak Bristol Bay Bristol Bay North 1 1950 0 0 0 0 0 0 0 0 0 0 0 0 0 27842 0 0 0 0 0 NA NA NA 163 Sockeye Togiak Bristol Bay Bristol Bay North 1 1951 0 0 0 0 0 0 0 0 98113 111 0 0 53034 8569 0 0 104 0 0 NA NA NA 163 Sockeye Togiak Bristol Bay Bristol Bay North 1 1952 0 0 0 0 0 0 0 152473 58318 0 0 0 8640 5917 0 0 0 0 0 NA NA NA 163 Sockeye Togiak Bristol Bay Bristol Bay North 1 1953 0 0 0 1114 0 0 0 31434 84135 0 0 0 8369 16206 0 0 0 0 0 NA NA NA 163 Sockeye Togiak Bristol Bay Bristol Bay North 1 1954 0 0 104 0 0 0 66 19804 145542 0 0 0 11957 17007 0 0 2006 0 0 NA NA NA 163 Sockeye Togiak Bristol Bay Bristol Bay North 1 1955 0 0 0 0 0 0 0 135517 189761 508 0 0 9022 38171 213 0 0 0 0 NA NA NA These values appear to not be real 0 values (no spawners) but instead indicate that there are no data available. We can find and replace these values with NA. brood$TotalEscapement[which(brood$TotalEscapement == 0)] &lt;- NA 2.5.2 Questionable data Now that we have confirmed all of our values are within the acceptable bounds defined above, we can search for additional outliers that may fall within the acceptable range numerically but still should not be used in analysis. One way we can check for these outliers is by calculating the total recruits to spawner ratio, (\\(R/S\\)), also called productivity. Typically stock productivity is less than 10. Values outside of these bounds could indicate that there are problems either with the age class recruit data, or the escapement data. First we calculate the total number of recruits by summing across all age class recruits in a row. Note that we need to use a customized function to calculate this sum, where if all values in the row are NA the total number of recruits is also NA (as opposed to 0). Otherwise, NA values are ignored in calculating the sum. The R/S value is calculated by dividing this value by the total number of spawners (TotalEscapement). nasum &lt;- function(x) if (all(is.na(x))) NA else sum(x,na.rm=T) brood$TotalRecruits &lt;- apply(select(brood, matches(&quot;R[0-9].[0-9]&quot;)), 1, nasum) Now we can look for outliers in productivity by examining at a boxplot of these values by stock. There are certainly some extreme outliers in this dataset. Although it would be beneficial to look at the data individually by stock, as a first pass for this exercise we can change the UseFlag column to 0, indicating that data should not be used in analysis, if the recruits/spawner ratio is greater than 25. We will also change the UseFlag to 0 if either the TotalRecruits or TotalEscapement columns are NA. brood$UseFlag[which(brood$TotalRecruits/brood$TotalEscapement &gt; 25)] &lt;- 0 brood$UseFlag[is.na(brood$TotalRecruits) == T | is.na(brood$TotalEscapement) == T] &lt;- 0 2.6 Analysis Now that we’ve done all of the work to clean and integrate all of the individual brood tables into a single data.frame, we’re ready to start exploring how the regions, sub-regions, and individual stocks might differ. A very common use of brood table data is to produce run size forecasts using the information we obtained by splitting returns into individual age classes. This approach revolves around the idea that productivity (\\(R/S\\)) for each age class is likely to be relatively more correlated within brood years than across brood years and that adults of some species of salmon (e.g., sockeye) have multiple return years for a given brood year. This lets us borrow information from the absolute number of earlier-returning (e.g., 1.2) individuals to make a guess about how many older individuals from that same brood year will return. Below, I show two sets of scatterplots of the number of age 1.3 fish against the number of age 1.2 fish. Each panel is overlaid with regressions about the origin. 2.6.1 Relationship between 1.2s and 1.3s by Region It is possible to add a trend line to the plot. In this case we used a linear model, and we used geom_smoth with method “lm”, only fitting the slope and forcing the intercept to be 0 (formula = “y ~ 0 + x”). The \\(R^2\\) value for the correlation is shown in each plot. formula &lt;- y ~ x + 0 ggplot(brood_s, aes(R1.2/1000, R1.3/1000)) + geom_point(size = 0.3) + geom_smooth(method = &quot;lm&quot;, formula = &quot;y ~ 0 + x&quot;, se = FALSE) + xlab(&quot;R1.2 x 1,000&quot;)+ylab(&quot;R1.3 x 1,000&quot;)+ stat_poly_eq(aes(label = paste(..rr.label..)), label.x.npc = &quot;right&quot;, label.y.npc = &quot;bottom&quot;, formula = formula, parse = TRUE, size = 3) + facet_wrap(~ Region, scales = &quot;free&quot;) + theme(strip.text.y = element_text(angle = 0), axis.text.x = element_text(angle = 90, hjust = 1)) Note that the scales are not fixed so comparing slopes across panels does not make sense. To compare the slopes in in graph we should fix y and x axis or export the slopes from the linear model. 2.6.2 Relationship between 1.2s and 1.3s by Region This section makes the same plots as before. Now instead of making them all in the same image using the option facet_wrap, we define a loop that goes region by region, fit a linear model forcing the intercept to be 0, and then create the plot. Because this is very repetitive, we only select 3 Regions to be plotted. We use \\(R^2\\) obtained with the function lm to change the color from blue to red, if \\(&gt;0.5\\). region &lt;- unique(brood_s$Region) for (i in region[1:3]) { temp &lt;- filter(brood_s, Region == i) test &lt;- lm(R1.3 ~ 0 + R1.2, data = temp) r &lt;- summary(test)$r.squared color &lt;- &quot;red&quot; if(r&lt;0.5) {color &lt;- &quot;blue&quot;} p &lt;- ggplot(temp, aes(R1.2, R1.3)) + geom_point(size = 0.5) + ggtitle(unique(temp$Region), ) + geom_smooth(method = &quot;lm&quot;, formula = &quot;y ~ 0 + x&quot;, color = color, se = FALSE) + stat_fit_glance(method = &#39;lm&#39;, method.args = list(formula = &quot;y ~ 0 + x&quot;), geom = &#39;text&#39;, aes(label = paste(&quot;R^2 = &quot;, signif(r, digits = 2), sep = &quot;&quot;)), label.x.npc = &#39;right&#39;, label.y.npc = 0.35, size = 3) + theme(plot.title = element_text(size=&quot;16&quot;)) print(p) } 2.6.3 Relationship between 1.2s and 1.3s by Stock The same type of plot can easily be produced for a different grouping level. For this we selected 20 stocks in the brood table from a set of regions: ggplot(filter(brood_s, Region %in% c(&quot;Bristol Bay&quot;, &quot;Kodiak&quot;, &quot;PWS&quot;, &quot;AK Peninsula&quot;)), aes(R1.2/1000, R1.3/1000)) + geom_point(size = 0.5) + geom_smooth(method = &quot;lm&quot;, formula = &quot;y ~ 0 + x&quot;, se = FALSE) + facet_wrap(~ Stock, nrow = 5, ncol = 4, scales = &quot;free&quot;) + xlab(&quot;R1.2 x 1,000&quot;)+ylab(&quot;R1.3 x 1,000&quot;)+ theme(axis.text.y = element_text(angle = 0, size = 7), axis.text.x = element_text(angle = 15, hjust = 1, size = 7)) Note that the scales are not fixed so comparing slopes across panels does not make sense. 2.6.4 Age Diversity As an example on how to create and use a function in R, we ask how the diversity of age clases have changed over time for each region. For this we use the Simpson’s diveristy index, which measures the probability that two oragsnisms randomly selected from a sample will belong to the same age class. For this, the function simpson is defined as \\(D = 1-\\sum(\\frac{n}{N})^2\\), where \\(n\\) is the total number of organisms in the age class \\(i\\) and \\(N\\) the total number or organisms in all the age classes. This function takes a vector containing the number of individual in each age class and returns the Simpson diversity index \\(D\\). simpson &lt;- function(ages) { total &lt;- sum(ages, na.rm = T) res &lt;- 0 for (i in 1:length(ages)) { if (!is.na(ages[i])) { res &lt;- res + (ages[i] / total) ^ 2 } } return(1 - res) } A new column is created with the Simpson diversity index. brood_s$Simpson &lt;- NA for (i in 1:nrow(brood_s)) { brood_s$Simpson[i] &lt;- simpson(brood_s[i,9:30]) } We then plot Simpson diversity index over time. We included the linear equation in each plot. This information can be easily exported as a CSV file to be used in other analysis. ggplot(brood_s, aes(BroodYear, as.numeric(Simpson))) + geom_point(size = 0.1) + geom_smooth(method = &quot;lm&quot;, formula = &quot;y ~ x&quot;, se = FALSE) + stat_poly_eq(aes(label = ..eq.label..), formula = &quot;y ~ x&quot;, parse = TRUE, size = 3) + facet_wrap(~ Region, ncol=5, nrow=2) + theme(axis.text.y = element_text(angle = 0), axis.text.x = element_text(angle = 90, hjust = 1)) "],
["rstudio-and-gitgithub-setup.html", "3 RStudio and Git/GitHub Setup 3.1 Learning Objectives 3.2 Checking the RStudio environment 3.3 Setting up git", " 3 RStudio and Git/GitHub Setup 3.1 Learning Objectives In this lesson, you will learn: How to check to make sure your RStudio environment is set up properly for analysis How to set up git 3.2 Checking the RStudio environment 3.2.1 R Studio Version First, lets make sure everyone’s RStudio is up to date. Run the following in your RStudio console: RStudio.Version()$version If the output of this does not say 1.1.383, you should update your RStudio. Do this by selecting Help -&gt; Check for Updates and follow the prompts. 3.2.2 Package installation Run the following lines to check that all of the packages we need for the training are installed on your computer. packages &lt;- c(&quot;dataone&quot;, &quot;datapack&quot;, &quot;devtools&quot;, &quot;dplyr&quot;, &quot;DT&quot;, &quot;EML&quot;, &quot;ggplot2&quot;, &quot;ggpmisc&quot;, &quot;kableExtra&quot;, &quot;leaflet&quot;, &quot;readxl&quot;, &quot;tidyr&quot;) for (package in packages) { if (!(package %in% installed.packages())) { install.packages(package) } } rm(packages) #remove variables from workspace If you haven’t installed all of the packages, this will automatically start installing them. If they are installed, it won’t do anything. Next, create a new R Markdown (File -&gt; New File -&gt; R Markdown). If you have never made an R Markdown document before, a dialog box will pop up asking if you wish to install the required packages. Click yes. 3.3 Setting up git If you haven’t already, go to github.com and create an account. If you haven’t downloaded git already, you can download it here. Before using git, you need to tell it who you are, also known as setting the global options. The only way to do this is through the command line. Newer versions of RStudio have a nice feature where you can open a terminal window in your RStudio session. Do this by selecting Tools -&gt; Terminal -&gt; New Terminal. A terminal tab should now be open where your console usually is. To set the global options, type the following into the command prompt, with your actual name, and press enter: git config --global user.name &quot;Your Name&quot; Next, enter the following line, with the email address you used when you created your account on github.com: git config --global user.email &quot;yourEmail@emaildomain.com&quot; Note that these lines need to be run one at a time. Finally, check to make sure everything looks correct by entering this line, which will return the options that you have set. git config --global --list "],
["literate-analysis-with-rmarkdown.html", "4 Literate Analysis with RMarkdown 4.1 Learning Objectives 4.2 What is (R)Markdown? 4.3 What’s possible with RMarkdown 4.4 RMarkdown overview 4.5 Literate analysis with RMarkdown 4.6 Organizing a reproducible research folder", " 4 Literate Analysis with RMarkdown 4.1 Learning Objectives In this lesson, you will learn: How plain text, Markdown, and RMarkdown differ How to write reports using RMarkdown How to integrate RMarkdown into your projects How RMarkdown enables literate and reproducible analysis 4.2 What is (R)Markdown? Markdown is a text format that embeds formatting directives in plain text documents in a natural way that doesn’t interfere with naturally reading the document as a text document. This is conducive to preservation, as text documents are excellent archival formats. For example, here is the start of a markdown document: # Assessment report - _Division_: Commercial Fisheries - _Date_: 2017-10-10 Plain text is a great way to write information down. Plain text has some major advantages: Works regardless of what decade you’re trying to read the file, i.e. computers have and will continue to speak plain text but MS Word formats will continue to be a pain in the neck forever Works perfectly with version control software like git. MS Word does not. Easy to read, easy to write Can embed code right in the narrative for seamless reproducibility Markdown is a plain text format that allows for common typesetting features including: Text formatting (bold, italic, etc.) Links Tables In-lined images Code Markdown minimized the need for spending time typesetting, which is tedious and usually unnecessary. So if we want those kinds of things, we might want to use Markdown. RMarkdown is an extension to Markdown that allows several additional formatting directives, most notably code blocks that let you embed R code in your document. When the document is processed, the code blocks can be executed, so that code output like figures can be included directly in the formatted document. For example, here’s a code block that generates a simple graph of random data in R (don’t try this in Word!): library(ggplot2) ggplot(data.frame(x=rnorm(100), y=rnorm(100)), aes(x, y)) + geom_point() RMarkdown allows us to produce several kinds of output document (web, PDF, DOCX) based on this mix of Markdown and R code. This lets us write analyses in R as we already do but also write our reports/papers/etc. in R. Instead of the usual loop: Run analysis Copy and paste graphics and tables into Word (forget where stuff came from) Edit/update report in Word The loop becomes: Edit RMarkdown Generate fully reproducible report Good resources: http://rmarkdown.rstudio.com/ http://kbroman.org/knitr_knutshell/ 4.3 What’s possible with RMarkdown RMarkdown supports a number of formatting directives. Some of the more useful include: Headings (H1 - H6) Formatting (bold, italics, strikethrough, superscript1) Links: Reproducible Analysis Images Inline equations: \\(A = \\pi*r^{2}\\) Lists Tables Code blocks Citations For example, the RMarkdown Reference Guide by RStudio shows how several of these directives would be formatted: We will review each of these by building a an RMarkdown document from scratch in RStudio. 4.4 RMarkdown overview Let’s create a an RStudio project called sasap-examples to be used for the examples in this tutorial. From within RStudio choose File | New Project…, and create a project in a new directory: Then, from within RStudio choose File | New File | RMarkdown file, and name the file practice.Rmd Then Save the file as practice.Rmd. You’ll note that RStudio filled in some example text in the Rmd file to get you started. 4.4.1 Knitting the file To see the rendered version of the file, hit the Knit button on the top of the editing pane, which will produce the formatted view of the file: 4.4.2 Basic formatting Use __bold__, *italics*, and ~~strikethough~~, and many others. Use bold, italics, and strikethough, and many others. 4.4.3 Links Links can be simply pasted in to show the whole URL like https://dataone.org, or can be linked to a specific chunk of text, e.g., to the [DataONE](https://dataone.org) project. Links can be simply pasted in to show the whole URL like https://dataone.org, or can be linked to a specific chunk of text, e.g., to the DataONE project. 4.4.4 Images Images are like links, but prepended with an ! exclamation point. ![](images/r-logo.png) Note how relative paths can be used to locate the file in the images folder. 4.4.5 R Code Chunks Code chunks (also called fenced code blocks) are created using three sequential backticks to start the block, then the name of the interpreter to use to execute the code (usually r), and then three backticks to end the block. For example: ```{r chunk-name-no-spaces, eval=TRUE} x ## [1] 8 There are a large variety of chunk options for controlling the output. 4.4.6 Inline R expressions and equations Just type what you want, like an equation like $\\sqrt{2}$ that would be calculated with R as `r sqrt(2)`. Just type what you want, like an equation like \\(\\sqrt{2}\\) that would be calculated with R as 1.4142136. Formatting for equations uses MathJax to render even complicated equations nicely: \\[\\sum_{i=0}^n i^2 = \\frac{(n^2+n)(2n+1)}{6}\\] 4.4.7 Plots The power of code chunks in RMarkdown lies in the ability to generate graphs and figures right inline. You can use base graphics, or more advanced libraries like ggplot2 and leaflet. Base graphics: Just run plot(1:10) plot(1:10) You can also use the full featues of ggplot: library(ggplot2) ggplot(data.frame(x=rnorm(100), y=rnorm(100)), aes(x, y)) + geom_point() Customize output sizing with chunk options: fig.width, fig.height, etc. 4.4.8 Tables You can render a simple table inline: | x | y | z | |-----|-----|-----| | 1 | 2 | 3 | | 4 | 5 | 6 | x y z 1 2 3 4 5 6 However, more likely, you’ll want to format some data from a data frame as a table that is pageable, searchable, and sortable. One really nice way is to use DT::datatable(), like so: data(&quot;mpg&quot;) DT::datatable(mpg, caption=&quot;Mileage&quot;) If you search around, there are tons of ways to do this. The simplest way is with the kable function from the knitr package, which is bare-bones but effective: data(&quot;mpg&quot;) knitr::kable(head(mpg)) manufacturer model displ year cyl trans drv cty hwy fl class audi a4 1.8 1999 4 auto(l5) f 18 29 p compact audi a4 1.8 1999 4 manual(m5) f 21 29 p compact audi a4 2.0 2008 4 manual(m6) f 20 31 p compact audi a4 2.0 2008 4 auto(av) f 21 30 p compact audi a4 2.8 1999 6 auto(l5) f 16 26 p compact audi a4 2.8 1999 6 manual(m5) f 18 26 p compact 4.5 Literate analysis with RMarkdown The idea behind literate analysis is to interleave the narrative of a scientific project with its methods and code to seamlessly document the exact relationships among the prose, the figures and tables, the code that produced them, and the data that was used by the code. Literate analysis is the inverse of commenting your code, as it lets you turn your paper into a living, executable analysis. A literate analysis using RMarkdown is fully reproducible, in that everything needed to understand the analysis and reproduce it is directly in the document. When done well, a user should be able to trace from the raw data through all data cleaning, integration, analysis, modeing, and visualization steps that lead to the final set of figures and tables that are embedded directly in the narrative. While an RMarkdown document is a single, linear exposition of a project, you can still modularize your code with functions and packages to make the code both more readable and more more re-usable. While one can certainly define functions within an RMarkdown document, for most analyses of even moderate complexity, we recommend separating functions into separate files and packages that can be defined and reused. The package rrtools (Marwick 2017) provides a convenient convention for organizing code into reusable modules using R’s standard package layout. 4.6 Organizing a reproducible research folder The rrtools package creates a folder convention for keeping code, data, and research results organized. For a project named sasap_training, the rrtools package could be used to organize and structure the project in a standard way. To start a project with RRtools, use: rrtools::use_compendium(&quot;sasap_training&quot;) # Then, from within the created package, run: rrtools::use_readme_rmd() rrtools::use_analysis() which produces a new RStudio project with specific folders for each type of data and code for the analysis with the following layout: sasap_training ├── CONDUCT.md ├── CONTRIBUTING.md ├── DESCRIPTION ├── R │   ├── data_load.R │   ├── diversity_functions.R │   ├── stock_functions.R ├── README.Rmd ├── analysis │   ├── data │   │   ├── derived_data │   │   │   ├── salmon_asl_integrated.csv │   │   └── raw_data │   │   │   ├── salmon_asl_southeast.csv │   │   │   ├── salmon_asl_central.csv │   │   │   ├── salmon_asl_kuskokwim.csv │   ├── figures │   │   │   ├── figure_01.png │   ├── paper │   │   ├── journal-of-archaeological-science.csl │   │   ├── paper.Rmd │   │   └── references.bib └── salmonstock.Rproj In this structure, untouched raw data would go in the analysis/data/raw_data folder, re-usable code functions go in the R folder, and the Rmd file associated with the manuscript goes into analysis/paper/paper.Rmd along with associated bibliographic files. Note that the data in derived_data would ideally be produced by running the code in paper.Rmd, which would execute functions from data_load.R to create the derived data file, which in turn would be analyzed and visualized by other functions found in the R folder. In this way, the complete analysis from raw data through to a completely formatted manuscript would be represented by the paper.Rmd, with all data and dependencies fully specified. This is truly a reproducible computation. Further reading: (Marwick, Boettiger, and Mullen 2017) References "],
["version-control-with-git-and-github.html", "5 Version Control With git and GitHub 5.1 Learning Objectives 5.2 The problem with filenames 5.3 Version control and Collaboration using Git and GitHub 5.4 Let’s look at a GitHub repository 5.5 The Git lifecycle 5.6 Create a remote repository on GitHub 5.7 Working locally with Git via RStudio 5.8 On good commit messages 5.9 Collaboration and conflict free workflows 5.10 Exercise 5.11 Advanced topics", " 5 Version Control With git and GitHub Instructor: Matt Jones 5.1 Learning Objectives In this lesson, you will learn: Why git is useful for reproducible analysis How to use git to track changes to your work over time How to use GitHub to collaborate with others How to structure your commits so your changes are clear to others How to write effective commit messages 5.2 The problem with filenames Every file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when problems are discovered. Data files get combined together, then errors are fixed, and then they are split and combined again. In the course of a single analysis, one can expect thousands of changes to files. And yet, all we use to track this are simplistic filenames. You might think there is a better way, and you’d be right: version control. Version control systems help you track all of the changes to your files, without the spaghetti mess that ensues from simple file renaming. In version control systems like git, the system tracks not just the name of the file, but also its contents, so that when contents change, it can tell you which pieces went where. It tracks which version of a file a new version came from. So its easy to draw a graph showing all of the versions of a file, like this one: Version control systems assign an identifier to every version of every file, and track their relationships. They also allow branches in those versions, and merging those branches back into the main line of work. They also support having multiple copies on multiple computers for backup, and for collaboration. And finally, they let you tag particular versions, such that it is easy to return to a set of files exactly as they were when you tagged them. For example, the exact versions of data, code, and narrative that were used when a manuscript was submitted might be R2 in the graph above. 5.3 Version control and Collaboration using Git and GitHub Let’s distinguish between git and GitHub: git: version control software used to track files in a folder (a repository) git creates the versioned history of a repository GitHub: web site that allows users to store their git repositories and share them with others 5.4 Let’s look at a GitHub repository This screen shows the copy of a repository stored on GitHub, with its list of files, when the files and directories were last modified, and some information on who made the most recent changes. If we drill into the “commits” for the repository, we can see the history of changes made to all of the files. Looks like kellijohnson and seananderson were fixing things in June and July: And finally, if we drill into the changes made on June 13, we can see exactly what was changed in each file: Tracking these changes, how they relate to released versions of software and files is exactly what Git and GitHub are good for. And we will show how they can really be effective for tracking versions of scientific code, figures, and manuscripts to accomplish a reproducible workflow. 5.5 The Git lifecycle As a git user, you’ll need to understand the basic concepts associated with versioned sets of changes, and how they are stored and moved across repositories. Any given git repository can be cloned so that it exist both locally, and remotely. But each of these cloned repositories is simply a copy of all of the files and change history for those files, stored in git’s particular format. For our purposes, we can consider a git repository just a folder with a bunch of additional version-related metadata. In a local git-enabled folder, the folder contains a workspace containing the current version of all files in the repository. These working files are linked to a hidden folder containing the ‘Local repository’, which contains all of the other changes made to the files, along with the version metadata. So, when working with files using git, you can use git commands to indicate specifically which changes to the local working files should be staged for versioning (using the git add command), and when to record those changes as a version in the local repository (using the command git commit). The remaining concepts are involved in synchronizing the changes in your local repository with changes in a remote repository. The git push command is used to send local changes up to a remote repository (possibly on GitHub), and the git pull command is used to fetch changes from a remote repository and merge them into the local repository. git clone: to copy a whole remote repository to local git add (stage): notify git to track particular changes git commit: store those changes as a version git pull: merge changes from a remote repository to our local repository git push: copy changes from our local repository to a remote repository git status: determine the state of all files in the local repository git log: print the history of changes in a repository Those seven commands are the majority of what you need to successfully use git. But this is all super abstract, so let’s explore with some real examples. 5.6 Create a remote repository on GitHub Let’s start by creating a repository on GitHub, then we’ll edit some files. Log into GitHub Click the New repository button Name it sasap-test Create a README.md Set the LICENSE to Apache 2.0 You’ve now created your first repository! It has a couple of files that GitHub created for you, like the README.md file, and the LICENSE file, and the .gitignore file. For simple changes to text files, you can make edits right in the GitHub web interface. For example, navigate to the README.md file in the file listing, and edit it by clicking on the pencil icon. This is a regular Markdown file, so you can just add text, and when done, add a commit message, and hit the Commit changes button. Congratulations, you’ve now authored your first versioned commit. If you navigate back to the GitHub page for the repository, you’ll see your commit listed there, as well as the rendered README.md file. Let’s point out a few things about this window. It represents a view of the repository that you created, showing all of the files in the repository so far. For each file, it shows when the file was last modified, and the commit message that was used to last change each file. This is why it is important to write good, descriptive commit messages. In addition, the blue header above the file listing shows the most recent commit, along with its commit message, and its SHA identifer. That SHA identifier is the key to this set of versioned changes. If you click on the SHA identifier (810f314), it will display the set of changes made in that particular commit. In the next section we’ll use the GitHub URL for the GitHub repository you created to clone the repository onto your local machine so that you can edit the files in RStudio. To do so, start by copying the GitHub URL, which represents the repository location: 5.7 Working locally with Git via RStudio RStudio knows how to work with files under version control with Git, but only if you are working within an RStudio project folder. In this next section, we will clone the repository that you created on GitHub into a local repository as an RStudio project. Here’s what we’re going to do: Create the new project Inspect the Git tab and version history Commit a change to the README.md file Commit the changes that RStudio made Inspect the version history Add and commit an Rmd file Push these changes to GitHub View the change history on GitHub Create a New Project. Start by creating a New Project… in R Studio, select the Version Control option, and paste the GitHub URL that you copied into the field for the remote repository Repository URL. While you can name the local copy of the repository anything, its typical to use the same name as the GitHub repository to maintain the correspondence. You can choose any folder for your local copy, in my case I used my standard development folder. Once you hit `Create Project, a new RStudio windo will open with all of the files from the remote repository copied locally. Depending on how your version of RStudio is configured, the location and size of the panes may differ, but they should all be present, including a Git tab and the normal Files tab listing the files that had been created in the remote repository. You’ll not that there is one new file sasap-test.Rproj, and three files that we created earlier on GitHub (.gitignore, LICENSE, and README.md). In the Git tab, you’ll note that two files are listed. This is the status pane that shows the current modification status of all of the files in the repository. In this case, the .gitignore file is listed as M for Modified, and sasap-test.Rproj is listed with a ? ? to indicate that the file is untracked. This means that git has not stored any versions of this file, and knows nothing about the file. As you make version control decisions in RStudio, these icons will change to reflect the current version status of each of the files. Inspect the history. For now, let’s click on the History button in the Git tab, which will show the log of changes that occurred, and will be identical to what we viewed on GitHub. By clicking on each row of the history, you can see exactly what was added and changed in each of the two commits in this repository. Commit a README.md change. Next let’s make a change to the README.md file in RStudio. Add a new section, with a markdown block like this: ## Git from RStudio From within RStudio, we can perform the same versioning actions that we can in GitHub, and much more. Plus, we have the natural advantages of the programming IDE with code completion and other features to make our work easier. - Add files to version control - Commit changes - Push commits to GitHub Once you save, you’ll immediately see the README.md file show up in the Git tab, marks as a modification. You can select the file in the Git tab, and click Diff to see the differences that you saved (but which are not yet committed to your local repository). And here’s what the newly made changes look like compared to the original file. New lines are highlighted in green, while removed lines are in red. Commit the RStudio changes. To commit the changes you made to the README.md file, check the Staged checkbox next to the file (which tells Git which changes you want included in the commit), then provide a descriptive Commit message, and then click Commit. Note that some of the changes in the repository, namely .gitignore and sasap-test.Rproj are still listed as having not been committed. This means there are still pending changes to the repository. You can also see the note that says: Your branch is ahead of ‘origin/master’ by 1 commit. This means that we have committed 1 change in the local repository, but that commit has not yet been pushed up to the origin repository, where origin is the typical name for our remote repository on GitHub. So, let’s commit the remaining project files by staging them and adding a commit message. When finished, you’ll see that no changes remain in the Git tab, and the repository is clean. Inspect the history. Note that the message now says: Your branch is ahead of ‘origin/master’ by 2 commits. These 2 commits are the two we just made, and have not yet been pushed to GitHub. By clicking on the History button, we can see that there are now a total of four commits in the local repository (while there had only been two on GitHub). Push these changes to GitHub. Now that everything has been changed as desired locally, you can push the changes to GitHub using the Push button. This will prompt you for your GitHub username and password, and upload the changes, leaving your repository in a totally clean and synchronized state. When finished, looking at the history shows all four commits, including the two that were done on GitHub and the two that were done locally on RStudio. And note that the labels indicate that both the local repository (HEAD) and the remote repository (origin/HEAD) are pointing at the same version in the history. So, if we go look at the commit history on GitHub, all the commits will be shown there as well. 5.8 On good commit messages Clearly, good documentation of what you’ve done is critical to making the version history of your repository meaningful and helpful. Its tempting to skip the commit message altogether, or to add some stock blurd like ‘Updates’. Its better to use messages that will be helpful to your future self in deducing not just what you did, but why you did it. Also, commit messaged are best understood if they follow the active verb convention. For example, you can see that my commit messages all started with a past tense verb, and then explained what was changed. While some of the changes we illustrated here were simple and so easily explained in a short phrase, for more complext changes, its best to provide a more complete message. The convention, however, is to always have a short, terse first sentence, followed by a more verbose explanation of the details and rationale for the change. This keeps the high level details readable in the version log. I can’t count the number of times I’ve looked at the commit log from 2, 3, or 10 years prior and been so grateful for diligence of my past self and collaborators. 5.9 Collaboration and conflict free workflows Up to now, we have been focused on using Git and GitHub for yourself, which is a great use. But equally powerful is to share a GitHib repository with other researchers so that you can work on code, analyses, and models together. When working together, you will need to pay careful attention to the state of the remote repository to avoid and handle merge conflicts. A merge conflict occurs when two collaborators make two separate commits that change the same lines of the same file. When this happens, git can’t merge the changes together automatically, and will give you back an error asking you to resolve the conflict. Don’t be afraid of merge conflicts, they are pretty easy to handle. and there are some great guides. That said, its truly painless if you can avoid merge conflicts in the first place. You can minimize conflicts by: Ensure that you pull down changes just before you commit Ensures that you have the most recent changes But you may have to fix your code if conflict would have occurred Coordinate with your collaborators on who is touching which files You still need to comunicate to collaborate 5.10 Exercise Use RStudio to add a new RMarkdown file to your sasap-test repository, build a basic structure for the file, and then save it. Next, stage and commit the file locally, and push it up to GitHub. 5.11 Advanced topics There’s a lot we haven’t covered in this brief tutorial. There are some great and much longer tutorials that cover advanced topics, such as: Using git on the command line Resolving conflicts Branching and merging Pull requests versus direct contributions for collaboration Using .gitignore to protect sensitive data GitHub Issues and why they are useful and much, much more Try Git is a great interactive tutorial Software Carpentry Version Control with Git Codecademy Learn Git (some paid) "],
["data-modeling-tidy-data.html", "6 Data Modeling &amp; Tidy Data 6.1 Learning Objectives 6.2 Benefits of relational data systems 6.3 Data Organization 6.4 Multiple tables 6.5 Inconsistent observations 6.6 Inconsistent variables 6.7 Marginal sums and statistics 6.8 Good enough data modeling 6.9 Primary and Foreign Keys 6.10 Entity-Relationship Model (ER) 6.11 Merging data 6.12 Simple Guidelines for Effective Data 6.13 Data modeling exercise 6.14 Related resources", " 6 Data Modeling &amp; Tidy Data 6.1 Learning Objectives Understand basics of relational data models Learn how to design and create effective data tables 6.2 Benefits of relational data systems Powerful search and filtering Handle large, complex data sets Enforce data integrity Decrease errors from redundant updates 6.3 Data Organization 6.4 Multiple tables 6.5 Inconsistent observations 6.6 Inconsistent variables 6.7 Marginal sums and statistics 6.8 Good enough data modeling 6.8.1 Denormalized data Observations about different entities combined In the above example, each row has measurements about both the site at which observations occurred, as well as observations of two individuals of possibly different species found at that site. This is not normalized data. People often refer to this as wide format, because the observatiions are spread across a wide number of columns. Note that, should one encounter a new species in the survey, we wold have to add new columns to the table. This is difficult to analyze, understand, and maintain. 6.8.2 Tabular data Observations. A better way to model data is to organize the observations about each type of entity in its own table. This results in: Separate tables for each type of entity measured Each row represents a single observed entity Observations (rows) are all unique This is normalized data (aka tidy data) Variables. In addition, for normalized data, we expect the variables to be organized such that: All values in a column are of the same type All columns pertain to the same observed entity (e.g., row) Here’s an example of tidy (normalized) data in which the top table is the collection of observations about individuals of several species, and the bottom table are the observations containing properties of the sites at which the species occurred. 6.9 Primary and Foreign Keys When one has normalized data, we often use unique identifiers to reference particular observations, which allows us to link across tables. Two types of identifiers are common within relational data: Primary Key: unique identifier for each observed entity, one per row Foreign Key: reference to a primary key in another table (linkage) For example, in the second table below, the site column is the primary key of that table, because it uniquely identifies each row of the table as a unique observation of a site. Inthe first table, however, the site column is a foreign key that references the primary key from the second table. This linkage tells us that the first height measurement for the DAPU observation occurred at the site with the name Taku. 6.10 Entity-Relationship Model (ER) An Entity-Relationship model allows us to compactly draw the structure of the tables in a relational database, including the primary and foreign keys in the tables. In the above model, one can see that each site in the SITES table must have one or more observations in the PLOTOBS table, whereas each PLOTOBS has one and only one SITE. 6.11 Merging data Frequently, analysis of data will require merging these separately managed tables back together. There are multiple ways to join the observations in two tables, based on how the rows of one table are merged with the rows of the other. When conceptualizing merges, one can think of two tables, one on the left and one on the right. The most common (and often useful) join is when you merge the subset of the rows from the left table with their matching rows from the right table: this is called an INNER JOIN. Other types of join are possible as well, including a LEFT JOIN and a RIGHT JOIN. In the figure above, the blue regions show the set of rows that are included in the result. For the INNER join, the rows returned are all rows in A that have a matching row in B. 6.12 Simple Guidelines for Effective Data Design to add rows, not columns Each column one type Eliminate redundancy Uncorrected data file Header line Nonproprietary formats Descriptive names No spaces Borer et al. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America. 6.13 Data modeling exercise Break into groups, 1 per table To demonstrate, we’ll be working with a tidied up version of a dataset from ADF&amp;G containing commercial catch data from 1878-1997. The dataset and reference to the original source can be viewed at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2. That site includes metadata describing the full data set, including column definitions. Here’s the first catch table: And here’s the region_defs table: Draw an ER model for the tables Indicate the primary and foreign keys 6.14 Related resources Borer et al. 2009. Some Simple Guidelines for Effective Data Management. Bulletin of the Ecological Society of America. Software Carpentry SQL tutorial Tidy Data "],
["data-cleaning-and-manipulation.html", "7 Data Cleaning and Manipulation 7.1 Learning Objectives 7.2 Introduction 7.3 Setup 7.4 Selecting/removing columns: select() 7.5 Changing shape: gather() and spread() 7.6 Renaming columns with rename() 7.7 Adding columns: mutate() 7.8 group_by and summarise 7.9 Filtering rows: filter() 7.10 Sorting your data: arrange() 7.11 Joins in dplyr 7.12 separate() and unite() 7.13 Summary", " 7 Data Cleaning and Manipulation 7.1 Learning Objectives In this lesson, you will learn: What the Split-Apply-Combine strategy is and how it applies to data The difference between wide vs. tall table formats and how to convert between them How to use dplyr and tidyr to clean and manipulate data for analysis How to join multiple data.frames together usingn dplyr 7.2 Introduction The data we get to work with are rarely, if ever, in the format we need to do our analyses. It’s often the case that one package requires data in one format, while another package requires the data to be in another format. To be efficient analysts, we should have good tools for reformatting data for our needs so we can do our actual work like making plots and fitting models. The dplyr and tidyr R packages provide a fairly complete and extremely powerful set of functions for us to do this reformatting quickly and learning these tools well will greatly increase your efficiency as an analyst. Analyses take many shapes, but they often conform to what is known as the Split-Apply-Combine strategy. This strategy follows a usual set of steps: Split: Split the data into logical groups (e.g., area, stock, year) Apply: Calculate some summary statistic on each group (e.g. mean total length by year) Combine: Combine the groups back together into a single table Figure 1: diagram of the split apply combine strategy As shown above (Figure 1), our original table is split into groups by year, we calculate the mean length for each group, and finally combine the per-year means into a single table. dplyr provides a fast and powerful way to express this. Let’s look at a simple example of how this is done: Assuming our length data is already loaded in a data.frame called length_data: year length_cm 1991 5.673318 1991 3.081224 1991 4.592696 1992 4.381523 1992 5.597777 1992 4.900052 1992 4.139282 1992 5.422823 1992 5.905247 1992 5.098922 We can do this calculation using dplyr like this: length_data %&gt;% group_by(year) %&gt;% summarise(mean_length_cm = mean(length_cm)) Another exceedingly common thing we need to do is “reshape” our data. Let’s look at an example table that is in what we will call “wide” format: site 1990 1991 … 1993 gold 100 118 … 112 lake 100 118 … 112 … … … … … dredge 100 118 … 112 You are probably quite familiar with data in the above format, where values of the variable being observed are spread out across columns (Here: columns for each year). Another way of describing this is that there is more than one measurement per row. This wide format works well for data entry and sometimes works well for analysis but we quickly outgrow it when using R. For example, how would you fit a model with year as a predictor variable? In an ideal world, we’d be able to just run: lm(length ~ year) But this won’t work on our wide data because lm needs length and year to be columns in our table. Or how would we make a separate plot for each year? We could call plot one time for each year but this is tedious if we have many years of data and hard to maintain as we add more years of data to our dataset. The tidyr package allows us to quickly switch between wide format and what is called tall format using the gather function: site_data %&gt;% gather(year, length, -site) site year length gold 1990 101 lake 1990 104 dredge 1990 144 … … … dredge 1993 145 In this lesson we’re going to walk through the functions you’ll most commonly use from the dplyr and tidyr packages: dplyr mutate() group_by() summarise() select() filter() arrange() left_join() rename() tidyr gather() spread() extract() separate() 7.3 Setup Let’s start going over the most common functions you’ll use from the dplyr package. To demonstrate, we’ll be working with a tidied up version of a dataset from ADF&amp;G containing commercial catch data from 1878-1997. The dataset and reference to the original source can be found at its public archive: https://knb.ecoinformatics.org/#view/df35b.304.2. First, let’s load dplyr and tidyr: library(dplyr) library(tidyr) Then let’s read in the data and take a look at it: catch_df &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) head(catch_df) ## Region Year Chinook Sockeye Coho Pink Chum All notesRegCode ## 1 SSE 1886 0 5 0 0 0 5 ## 2 SSE 1887 0 155 0 0 0 155 ## 3 SSE 1888 0 224 16 0 0 240 ## 4 SSE 1889 0 182 11 92 0 285 ## 5 SSE 1890 0 251 42 0 0 292 ## 6 SSE 1891 0 274 24 0 0 298 Note: I copied the URL from the Download button on https://knb.ecoinformatics.org/#view/df35b.304.2 This dataset is relatively clean and easy to interpret as-is. But while it may be clean, it’s in a shape that makes it hard to use for some types of analyses so we’ll want to fix that first. 7.4 Selecting/removing columns: select() The first issue is the extra columns All and notesRegCode. Let’s remove those columns: catch_df %&gt;% select(-All, -notesRegCode) %&gt;% head() ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 Much beter! In the above example, we pass unquoted column names preceded by minus (-) signs to specify which columns we dont want. select() also allows us to specify which columns we do want: catch_df %&gt;% select(Region, Year, Chinook, Sockeye, Coho, Pink, Chum) %&gt;% head() ## Region Year Chinook Sockeye Coho Pink Chum ## 1 SSE 1886 0 5 0 0 0 ## 2 SSE 1887 0 155 0 0 0 ## 3 SSE 1888 0 224 16 0 0 ## 4 SSE 1889 0 182 11 92 0 ## 5 SSE 1890 0 251 42 0 0 ## 6 SSE 1891 0 274 24 0 0 Let’s save our changes by overwriting the catch_df variable: catch_df &lt;- catch_df %&gt;% select(-All, -notesRegCode) 7.5 Changing shape: gather() and spread() The next issue is that the data are in a wide format and, we want the data in a tall format instead. gather() from the tidyr package helps us do just this conversion: catch_df &lt;- catch_df %&gt;% gather(species, catch, -Region, -Year) head(catch_df) ## Region Year species catch ## 1 SSE 1886 Chinook 0 ## 2 SSE 1887 Chinook 0 ## 3 SSE 1888 Chinook 0 ## 4 SSE 1889 Chinook 0 ## 5 SSE 1890 Chinook 0 ## 6 SSE 1891 Chinook 0 The syntax we used above for gather() might be a bit confusing so let’s look at an annotated diagram: annotated gather code The first two arguments to gather() are the names of new columns that will be created and the other arguments with - symbols in front of them are columns to keep around in this process. The opposite of gather(), spread(), works in a similar declarative fashion: catch_df %&gt;% spread(species, catch) %&gt;% head() ## Region Year Chinook Chum Coho Pink Sockeye ## 1 ALU 1911 0 0 0 0 9 ## 2 ALU 1912 0 0 0 0 0 ## 3 ALU 1913 0 0 0 0 0 ## 4 ALU 1914 0 0 0 0 0 ## 5 ALU 1915 0 0 0 0 0 ## 6 ALU 1916 0 0 1 180 76 7.6 Renaming columns with rename() If you scan through the data, you may notice the values in the catch column are very small (these are supposed to be annual catches). If we look at the metadata we can see that the catch column is in thousands of fish so let’s convert it before moving on. Let’s first rename the catch column to be called catch_thousands: catch_df &lt;- catch_df %&gt;% rename(catch_thousands = catch) head(catch_df) ## Region Year species catch_thousands ## 1 SSE 1886 Chinook 0 ## 2 SSE 1887 Chinook 0 ## 3 SSE 1888 Chinook 0 ## 4 SSE 1889 Chinook 0 ## 5 SSE 1890 Chinook 0 ## 6 SSE 1891 Chinook 0 7.7 Adding columns: mutate() Now let’s create a new column called catch with units of fish (instead of thousands of fish): catch_df &lt;- catch_df %&gt;% mutate(catch = catch_thousands * 1000) You’ll notice that we get an error: Error in mutate_impl(.data, dots) : Evaluation error: non-numeric argument to binary operator. This is an extremely cryptic error – what is it telling us? These kinds of errors can be very hard to diagnose, but maybe the catch column isn’t quite what we are expecting. How could we find out? R provides a number of handy utility functions for quickly summarizing a large table: summary(catch_df) ## Region Year species catch_thousands ## Length:8540 Min. :1878 Length:8540 Length:8540 ## Class :character 1st Qu.:1922 Class :character Class :character ## Mode :character Median :1947 Mode :character Mode :character ## Mean :1946 ## 3rd Qu.:1972 ## Max. :1997 Exercise: What are some other ways (functions) we could’ve found out what our problem was? Notice in the above output that the catch_thousands column shows up as Class :character. That seems wrong since catch should be whole numbers (in R, these show up as integers). Let’s try to convert the values to integers and see what happens: catch_integers &lt;- as.integer(catch_df$catch_thousands) ## Warning: NAs introduced by coercion We get an error “NAs introduced by coercion” which is R telling us that it couldn’t convert every value to an integer and, for those values it couldn’t convert, it put an NA in its place. This is behavior we commonly experience when cleaning datasets and it’s important to have the skills to deal with it when it crops up. We can find out which values are NAs with a combination of is.na() and which(): which(is.na(catch_integers)) ## [1] 401 It looks like the 401st value is the problem. Let’s look at the 401s row of the catch data.frame: catch_df[401,] ## Region Year species catch_thousands ## 401 GSE 1955 Chinook I Well that’s odd: The value in catch_thousands is I which is isn’t even a number. It turns out that this dataset is from a PDF which was automatically converted into a CSV and this value of I is actually a 1. Let’s fix it: catch_df &lt;- catch_df %&gt;% mutate(catch_thousands = ifelse(catch_thousands == &quot;I&quot;, 1, catch_thousands), catch_thousands = as.integer(catch_thousands)) Note that, in the above pipeline call to mutate(), we mutate catch_thousands twice. This works because mutate() processes each of the mutations in a step-wise fashion so the results of one mutation are available for the next. Now let’s try our conversion again: catch_df &lt;- catch_df %&gt;% mutate(catch = catch_thousands * 1000) summary(catch_df) ## Region Year species catch_thousands ## Length:8540 Min. :1878 Length:8540 Min. : 0.0 ## Class :character 1st Qu.:1922 Class :character 1st Qu.: 0.0 ## Mode :character Median :1947 Mode :character Median : 36.0 ## Mean :1946 Mean : 873.2 ## 3rd Qu.:1972 3rd Qu.: 377.2 ## Max. :1997 Max. :53676.0 ## catch ## Min. : 0 ## 1st Qu.: 0 ## Median : 36000 ## Mean : 873249 ## 3rd Qu.: 377250 ## Max. :53676000 Looks good, no warnings! Now let’s remove the catch_thousands column for now since we don’t need it: catch_df &lt;- catch_df %&gt;% select(-catch_thousands) head(catch_df) ## Region Year species catch ## 1 SSE 1886 Chinook 0 ## 2 SSE 1887 Chinook 0 ## 3 SSE 1888 Chinook 0 ## 4 SSE 1889 Chinook 0 ## 5 SSE 1890 Chinook 0 ## 6 SSE 1891 Chinook 0 We’re now ready to start analyzing the data. 7.8 group_by and summarise As I outlined in the Introduction, dplyr lets us employ the Split-Apply-Combine strategy and this is exemplified through the use of the group_by() and summarise() functions: catch_df %&gt;% group_by(Region) %&gt;% summarise(mean(catch)) ## # A tibble: 18 x 2 ## Region `mean(catch)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 ALU 40384. ## 2 BER 16373. ## 3 BRB 2709796. ## 4 CHG 315487. ## 5 CKI 683571. ## 6 COP 179223. ## 7 GSE 133841. ## 8 KOD 1528350 ## 9 KSK 67642. ## 10 KTZ 18836. ## 11 NOP 229493. ## 12 NRS 51503. ## 13 NSE 1825021. ## 14 PWS 1419237. ## 15 SOP 1110942. ## 16 SSE 3184661. ## 17 YAK 91923. ## 18 YUK 68646. Exercise: Find another grouping and statistic to calculate for each group. Exercise: Find out if you can group by multiple variables. Another common use of group_by() followed by summarize() is to count the number of rows in each group. We have to use a special function from dplyr, n(). catch_df %&gt;% group_by(Region) %&gt;% summarize(n = n()) ## # A tibble: 18 x 2 ## Region n ## &lt;chr&gt; &lt;int&gt; ## 1 ALU 435 ## 2 BER 510 ## 3 BRB 570 ## 4 CHG 550 ## 5 CKI 525 ## 6 COP 470 ## 7 GSE 410 ## 8 KOD 580 ## 9 KSK 425 ## 10 KTZ 415 ## 11 NOP 460 ## 12 NRS 185 ## 13 NSE 575 ## 14 PWS 545 ## 15 SOP 450 ## 16 SSE 560 ## 17 YAK 480 ## 18 YUK 395 7.9 Filtering rows: filter() filter() is the verb we use to filter our data.frame to rows matching some condition. It’s similar to subset() from base R. Let’s go back to our original data.frame and do some filter()ing: catch_df %&gt;% filter(Region == &quot;SSE&quot;) %&gt;% head() # head() Show just the first n (default: 6) rows ## Region Year species catch ## 1 SSE 1886 Chinook 0 ## 2 SSE 1887 Chinook 0 ## 3 SSE 1888 Chinook 0 ## 4 SSE 1889 Chinook 0 ## 5 SSE 1890 Chinook 0 ## 6 SSE 1891 Chinook 0 Exercise: Filter to just catches of over one million fish. Exercise: Filter to just SSE Chinook 7.10 Sorting your data: arrange() arrange() is how we sort the rows of a data.frame. In my experience, I use arrange() in two common cases: When I want to calculate a cumulative sum (with cumsum()) so row order matters When I want to display a table (like in an .Rmd document) in sorted order Let’s re-calculate mean catch by region, and then arrange() the output by mean catch: catch_df %&gt;% group_by(Region) %&gt;% summarise(mean_catch = mean(catch)) %&gt;% arrange(mean_catch) ## # A tibble: 18 x 2 ## Region mean_catch ## &lt;chr&gt; &lt;dbl&gt; ## 1 BER 16373. ## 2 KTZ 18836. ## 3 ALU 40384. ## 4 NRS 51503. ## 5 KSK 67642. ## 6 YUK 68646. ## 7 YAK 91923. ## 8 GSE 133841. ## 9 COP 179223. ## 10 NOP 229493. ## 11 CHG 315487. ## 12 CKI 683571. ## 13 SOP 1110942. ## 14 PWS 1419237. ## 15 KOD 1528350 ## 16 NSE 1825021. ## 17 BRB 2709796. ## 18 SSE 3184661. The default sorting order of arrange() is to sort in ascending order. To reverse the sort order, wrap the column name inside the desc() function: catch_df %&gt;% group_by(Region) %&gt;% summarise(mean_catch = mean(catch)) %&gt;% arrange(desc(mean_catch)) ## # A tibble: 18 x 2 ## Region mean_catch ## &lt;chr&gt; &lt;dbl&gt; ## 1 SSE 3184661. ## 2 BRB 2709796. ## 3 NSE 1825021. ## 4 KOD 1528350 ## 5 PWS 1419237. ## 6 SOP 1110942. ## 7 CKI 683571. ## 8 CHG 315487. ## 9 NOP 229493. ## 10 COP 179223. ## 11 GSE 133841. ## 12 YAK 91923. ## 13 YUK 68646. ## 14 KSK 67642. ## 15 NRS 51503. ## 16 ALU 40384. ## 17 KTZ 18836. ## 18 BER 16373. 7.11 Joins in dplyr So now that we’re awesome at manipulating single data.frames, where do we go from here? Manipulating multiple data.frames. If you’ve ever used a database, you may have heard of or used what’s called a “join”, which allows us to to intelligently merge two tables together into a single table based upon a shared column between the two. We’ve already covered joins in Data Modeling &amp; Tidy Data so let’s see how it’s done with dplyr. The dataset we’re working with, https://knb.ecoinformatics.org/#view/df35b.304.2, contains a second CSV which has the definition of each Region code. This is a really common way of storing auxiliary information about our dataset of interest (catch) but, for analylitcal purposes, we often want them in the same data.frame. Joins let us do that easily: region_defs &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) catch_df %&gt;% group_by(Region) %&gt;% summarise(total_catch = sum(catch)) %&gt;% left_join(region_defs, by = c(&quot;Region&quot; = &quot;code&quot;)) ## # A tibble: 18 x 6 ## Region total_catch mgmtArea areaClass regionCode notes ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 ALU 17567000 Aleutian Isla… subarea 4 &quot;&quot; ## 2 BER 8350000 Bering River … subarea 2 &quot;&quot; ## 3 BRB 1544584000 Bristol Bay M… mgmtArea 2 &quot;&quot; ## 4 CHG 173518000 Chignik Manag… mgmtArea 4 &quot;&quot; ## 5 CKI 358875000 Cook Inlet Ma… mgmtArea 2 Cook Inlet incl… ## 6 COP 84235000 Copper River … subarea 2 &quot;&quot; ## 7 GSE 54875000 Unallocated S… mgmtArea 1 &quot;Included are S… ## 8 KOD 886443000 Kodiak Manage… mgmtArea 4 &quot;&quot; ## 9 KSK 28748000 Kuskokwim Man… mgmtArea 3 &quot;&quot; ## 10 KTZ 7817000 Kotzebue Mana… mgmtArea 3 &quot;&quot; ## 11 NOP 105567000 NorthPeninsua… subarea 4 &quot;&quot; ## 12 NRS 9528000 Norton Sound … mgmtArea 3 &quot;&quot; ## 13 NSE 1049387000 Northern Sout… mgmtArea 1 Northern Southe… ## 14 PWS 773484000 Prince Willia… subarea 2 &quot;&quot; ## 15 SOP 499924000 South Peninsu… subarea 4 &quot;&quot; ## 16 SSE 1783410000 Southern Sout… mgmtArea 1 &quot;&quot; ## 17 YAK 44123000 Yakutat mgmtArea 1 &quot;&quot; ## 18 YUK 27115000 Yukon Managem… mgmtArea 3 &quot;&quot; Note: left_join and the other join functions can often guess the shared column name but, in this case, the column names differed between the two data.frames so we had to be explicit. Now our catches have the auxiliary information from the region definitions file alongside them. Note: dplyr provides a complete set of joins: inner, left, right, full, semi, anti, not just left_join. 7.12 separate() and unite() separate() and its complement, unite() allow us to easily split a single column into numerous (or numerous into a single). This can come in really handle when we have a date column and we want to group by year or month. Let’s make a new data.frame with fake data to illustrate this: dates_df &lt;- data.frame(date = c(&quot;5/24/1930&quot;, &quot;5/25/1930&quot;, &quot;5/26/1930&quot;, &quot;5/27/1930&quot;, &quot;5/28/1930&quot;), stringsAsFactors = FALSE) dates_df %&gt;% separate(date, c(&quot;month&quot;, &quot;day&quot;, &quot;year&quot;), &quot;/&quot;) ## month day year ## 1 5 24 1930 ## 2 5 25 1930 ## 3 5 26 1930 ## 4 5 27 1930 ## 5 5 28 1930 Exercise: Split the city column in the following data.frame into city and state_code columns: cities_df &lt;- data.frame(city = c(&quot;Juneau AK&quot;, &quot;Sitka AK&quot;, &quot;Anchorage AK&quot;), stringsAsFactors = FALSE) # Write your solution here unite() does just the reverse of separate(): dates_df %&gt;% separate(date, c(&quot;month&quot;, &quot;day&quot;, &quot;year&quot;), &quot;/&quot;) %&gt;% unite(date, month, day, year, sep = &quot;/&quot;) ## date ## 1 5/24/1930 ## 2 5/25/1930 ## 3 5/26/1930 ## 4 5/27/1930 ## 5 5/28/1930 Exercise: Use unite() on your solution above to combine the cities_df back to its original form with just one column, city: # Write your solution here 7.13 Summary We just ran through the various things we can do with dplyr and tidyr but if you’re wondering how this might look in a real analysis. Let’s look at that now: catch_df &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.302.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) region_defs &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/df35b.303.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) catch_df %&gt;% select(-All, -notesRegCode) %&gt;% gather(species, catch, -Region, -Year) %&gt;% mutate(catch = ifelse(catch == &quot;I&quot;, 1, catch)) %&gt;% mutate(catch = as.integer(catch)) %&gt;% group_by(Region) %&gt;% summarize(mean_catch = mean(catch)) %&gt;% left_join(region_defs, by = c(&quot;Region&quot; = &quot;code&quot;)) ## # A tibble: 18 x 6 ## Region mean_catch mgmtArea areaClass regionCode notes ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 ALU 40.4 Aleutian Isla… subarea 4 &quot;&quot; ## 2 BER 16.4 Bering River … subarea 2 &quot;&quot; ## 3 BRB 2710. Bristol Bay M… mgmtArea 2 &quot;&quot; ## 4 CHG 315. Chignik Manag… mgmtArea 4 &quot;&quot; ## 5 CKI 684. Cook Inlet Ma… mgmtArea 2 Cook Inlet inclu… ## 6 COP 179. Copper River … subarea 2 &quot;&quot; ## 7 GSE 134. Unallocated S… mgmtArea 1 &quot;Included are So… ## 8 KOD 1528. Kodiak Manage… mgmtArea 4 &quot;&quot; ## 9 KSK 67.6 Kuskokwim Man… mgmtArea 3 &quot;&quot; ## 10 KTZ 18.8 Kotzebue Mana… mgmtArea 3 &quot;&quot; ## 11 NOP 229. NorthPeninsua… subarea 4 &quot;&quot; ## 12 NRS 51.5 Norton Sound … mgmtArea 3 &quot;&quot; ## 13 NSE 1825. Northern Sout… mgmtArea 1 Northern Souther… ## 14 PWS 1419. Prince Willia… subarea 2 &quot;&quot; ## 15 SOP 1111. South Peninsu… subarea 4 &quot;&quot; ## 16 SSE 3185. Southern Sout… mgmtArea 1 &quot;&quot; ## 17 YAK 91.9 Yakutat mgmtArea 1 &quot;&quot; ## 18 YUK 68.6 Yukon Managem… mgmtArea 3 &quot;&quot; "],
["hands-on-clean-and-integrate-datasets.html", "8 Hands On: Clean and Integrate Datasets 8.1 Learning Objectives 8.2 Outline 8.3 High-level steps 8.4 Full solution", " 8 Hands On: Clean and Integrate Datasets 8.1 Learning Objectives In this lesson, you will: Clean and integrate two datasets using dplyr and tidyr Make use of previously-learned knowledge of dplyr and tidyr 8.2 Outline In this one hour block, you will load data from the following two datasets into R, ADFG salmon escapement data ADFG salmon escapment goal data and then clean, and integrate them together to answer a research question: Are Sockeye salmon escapement goals being met in recent years in Bristol Bay? Depending on your familiarity with dplyr and tidyr, you will probably want to look up how to do things. I suggest two strategies: Look back on the Data Cleaning and Manipulation lesson Use the official dplyr documentation Once you know what function to use, use R’s built-in help by prepending a ? to the function name and running that (e.g., run ?select to get help on the select function) 8.3 High-level steps The goal here is for you to have to come up with the functions to do the analysis with minimal guidance. This is supposed to be hard. Below is a set of high-level steps you can follow to answer our research question. After the list is a schematic of the steps in table form which I expect will be useful in guiding your code. Note: This need not be the exaxct order your code is written in. Load our two datasets Load the escapement goals CSV into R as a data.frame Visit https://knb.ecoinformatics.org/#data and search for “escapement goals” and choose the 2007-2015 dataset Click the following dataset: Andrew Munro and Eric Volk. 2017. Summary of Pacific Salmon Escapement Goals in Alaska with a Review of Escapements from 2007 to 2015. Knowledge Network for Biocomplexity. Right-click and copy address for the file MandV2016 Load the escapement counts CSV into R as a data.frame Visit https://knb.ecoinformatics.org/#data and search for ‘oceanak’ Click the following dataset: Alaska Department of Fish and Game. 2017. Daily salmon escapement counts from the OceanAK database, Alaska, 1921-2017. Knowledge Network for Biocomplexity. Right-click and copy address for the file ADFG_firstAttempt_reformatted.csv Clean Clean the escapement goals dataset Filter to just the Bristol Bay region and the Sockeye salmon species Check whether the column types are wrong and fix any issues (Hint: One column has the wrong type) Clean the escapement counts dataset Filter to just the Bristol Bay region and the Sockeye salmon species Filter to just stocks we have escapement goals for Create new columns for the year, month, and day so we can calculate total escapements by year and stock Calculate annual total escapements for each stock Integrate Join the escapement goal lower and upper bounds onto the annual total escapement counts (Hint: We don’t need all the columns) Analyze Make a table listing annual total escapements and whether they were in the escapement goal range or not Calculate the proportion of years, for each stock, total escapement was within the escapement goal range 8.3.1 Visual schematic of steps Make this: System Lower Upper Kvichak River 2000000 10000000 Naknek River 800000 2000000 Egegik River 800000 2000000 Ugashik River 500000 1400000 Wood River 700000 1800000 Igushik River 150000 400000 Nushagak River 260000 760000 Nushagak River 370000 900000 Togiak River 120000 270000 and then make this: Location Year Escapement Egegik River 2012 1233900 Egegik River 2013 1113630 Egegik River 2014 1382466 Egegik River 2015 2160792 Egegik River 2016 1837260 Igushik River 2012 193326 Igushik River 2013 387744 Igushik River 2014 340590 Igushik River 2015 651172 Igushik River 2016 469230 and join them together to make this: Location Year Escapement Lower Upper is_in_range Egegik River 2012 1233900 800000 2000000 TRUE Egegik River 2013 1113630 800000 2000000 TRUE Egegik River 2014 1382466 800000 2000000 TRUE Egegik River 2015 2160792 800000 2000000 FALSE Egegik River 2016 1837260 800000 2000000 TRUE Igushik River 2012 193326 150000 400000 TRUE Igushik River 2013 387744 150000 400000 TRUE Igushik River 2014 340590 150000 400000 TRUE Igushik River 2015 651172 150000 400000 FALSE Igushik River 2016 469230 150000 400000 FALSE 8.4 Full solution Warning: Spoilers! First we’ll load our packages: suppressPackageStartupMessages({ library(dplyr) library(tidyr) library(DT) # Just for display purposes }) Then download our two data files and save them as data.frames: # https://knb.ecoinformatics.org/#view/urn:uuid:8809a404-f6e1-46a2-91c8-f094c3814b47 # Search &quot;OceanAK&quot; esc &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/knb.92020.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) # https://knb.ecoinformatics.org/#view/urn:uuid:2c13bb02-6148-4ab6-8ad5-3a7c03f8642e # Search &quot;escapement goals&quot;, choose 2007-2015 dataset goals &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/knb.92014.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) First, we’ll clean up the escapement goals data.frame to have just the rows and columns we need and display it: bb_sockeye_goals &lt;- goals %&gt;% filter(Region == &quot;Bristol Bay&quot;, Species == &quot;Sockeye&quot;) %&gt;% mutate(Lower = as.integer(Lower)) %&gt;% select(System, Lower, Upper) %&gt;% drop_na() ## Warning in evalq(as.integer(Lower), &lt;environment&gt;): NAs introduced by ## coercion datatable(bb_sockeye_goals) Then we’ll clean up and summarize the escapement counts data.frame, join the escapement goals data.frame onto it, and calculate whether goals have been met: bb_sockeye_escapements &lt;- esc %&gt;% filter(SASAP.Region == &quot;Bristol Bay&quot;, Species == &quot;Sockeye&quot;, Location %in% bb_sockeye_goals$System) %&gt;% separate(sampleDate, c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;), sep = &quot;-&quot;) %&gt;% group_by(Location, Year) %&gt;% summarize(Escapement = sum(DailyCount)) %&gt;% left_join(bb_sockeye_goals %&gt;% select(System, Lower, Upper), by = c(&quot;Location&quot; = &quot;System&quot;)) %&gt;% mutate(is_in_range = Escapement &gt;= Lower &amp; Escapement &lt;= Upper) datatable(bb_sockeye_escapements) "],
["publication-graphics-with-ggplot2.html", "9 Publication Graphics with ggplot2 9.1 Learning Objectives 9.2 Overview 9.3 ggplot vs base vs lattice vs XYZ… 9.4 Setup 9.5 Geoms / Aesthetics 9.6 Setting plot limits 9.7 Scales 9.8 Facets 9.9 Plot customization w/ themes 9.10 Saving plots 9.11 Bonus: Multi-panel plots: Beyond facets 9.12 Bonus: Round two", " 9 Publication Graphics with ggplot2 9.1 Learning Objectives In this lesson, you will learn: The basics of the ggplot2 package How to use ggplot2’s theming abilities to create publication-grade graphics How to create multi-panel plots 9.2 Overview ggplot2 is a popular package for visualizing data in R. From the home page: ggplot2 is a system for declaratively creating graphics, based on The Grammar of Graphics. You provide the data, tell ggplot2 how to map variables to aesthetics, what graphical primitives to use, and it takes care of the details. It’s been around for years and has pretty good documentation and tons of example code around the web (like on StackOverflow). This lesson will introduce you to the basic components of working with ggplot2. 9.3 ggplot vs base vs lattice vs XYZ… R provides many ways to get your data into a plot. Three common ones are, “base graphics” (plot(), hist(), etc`) lattice ggplot2 All of them work! I use base graphics for simple, quick and dirty plots. I use ggplot2 for most everything else. ggplot2 excels at making complicated plots easy and easy plots simple enough. 9.4 Setup To demonstrate ggplot2, we’re going to work with two example datasets suppressPackageStartupMessages({ library(ggplot2) library(tidyr) library(dplyr) }) # https://knb.ecoinformatics.org/#view/urn:uuid:e05865d7-678d-4513-9061-2ab7d979f8e7 # Search &#39;permit value&#39; permits &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3Aa3c58bd6-481e-4c64-aa93-795df10a4664&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) 9.5 Geoms / Aesthetics Every graphic you make in ggplot2 will have at least one aesthetic and at least one geom (layer). The aesthetic maps your data to your geometry (layer). Your geometry specifies the type of plot we’re making (point, bar, etc.). ggplot(permits, aes(Value, StdDev)) + geom_point() What makes ggplot really powerful is how quickly we can make this plot visualize more aspects of our data. Coloring each point by class (compact, van, pickup, etc.) is just a quick extra bit of code: ggplot(permits, aes(Value, StdDev, color = Gear)) + geom_point() Aside: How did I know to write color = class? aes will pass its arguments on to any geoms you use and we can find out what aesthetic mappings geom_point takes with ?geom_point (see section “Aesthetics”) Exercise: Find another aesthetic mapping geom_point can take and add add it to the plot. What if we just wanted the color of the points to be blue? Maybe we’d do this: ggplot(permits, aes(Value, StdDev, color = &quot;blue&quot;)) + geom_point() Well that’s weird – why are the points red? What happened here? This is the difference between setting and mapping in ggplot. The aes function only takes mappings from our data onto our geom. If we want to make all the points blue, we need to set it inside the geom: ggplot(permits, aes(Value, StdDev)) + geom_point(color = &quot;blue&quot;) Exercise: Using the aesthetic you discovered and tried above, set another aesthetic onto our points. Sizing each point by the range of the permit values is only a small change to the code: ggplot(permits, aes(Value, StdDev, color = Gear, size = Range)) + geom_point() So it’s clear we can make scatter and bubble plots. What other kinds of plots can we make? (Hint: Tons) Let’s make a histogram: ggplot(permits, aes(Value)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 14 rows containing non-finite values (stat_bin). You’ll see with a warning (red text): stat_bin() using bins = 30. Pick better value with binwidth. ggplot2 can calculate statistics on our data such as frequencies and, in this case, it’s doing that on our hwy column with the stat_bin function. Binning data requires choosing a bin size and the choice of bin size can completely change our histogram (possibly resulting in misleading conclusions about how the values are distributed). We might want to change the bins argument in this case to something narrower: ggplot(permits, aes(Value)) + geom_histogram(binwidth = 1e4) ## Warning: Removed 14 rows containing non-finite values (stat_bin). Exercise: Find an aesthetic geom_histogram supports and try it out. I’m a big fan of box plots and ggplot2 can plot these too: ggplot(permits, aes(Gear, Value)) + geom_boxplot() Another type of visualization I use a lot for seeing my distributions is the violin plot: permits_ci &lt;- permits %&gt;% filter(Region == &quot;Cook Inlet&quot;) ggplot(permits_ci, aes(Gear, Value)) + geom_violin() So far we’ve made really simple plots: One geometry per plot. Let’s layer multiple geometries on top of one another to show the raw points on top of the violins: ggplot(permits_ci, aes(Gear, Value)) + geom_violin() + geom_point(shape = 1, position = &quot;jitter&quot;) Some geoms can do even more than just show us our data. ggplot2 also helps us do some quick-and-dirty modeling: ggplot(permits, aes(Value, StdDev)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Notice the mesage in red text geom_smooth() using method = 'loess' geom_smooth defaulted here to using a LOESS smoother. But geom_smooth() is pretty configurable. Here we set the method to lm instead of the default loess: ggplot(permits, aes(Value, StdDev)) + geom_point() + geom_smooth(method = &quot;lm&quot;) More on geoms here: http://ggplot2.tidyverse.org/reference/index.html#section-layer-geoms 9.6 Setting plot limits Plot limits can be controlled one of three ways: Filter the data (because limits are auto-calculated from the data ranges) Set the limits argument on one or both scales Set the xlim and ylim arguments in coord_cartesian() Let’s show this with an example plot: permits_se_seine &lt;- permits %&gt;% filter(Gear == &quot;Purse Seine&quot;, Region == &quot;Southeast&quot;) ggplot(permits_se_seine, aes(Year, Value)) + geom_point() + geom_line() Let’s make the Y axis start from 0: ggplot(permits_se_seine, aes(Year, Value)) + geom_point() + geom_line() + scale_y_continuous(limits = c(0, max(permits_se_seine$Value))) Let’s say, for some reason, we wanted to only show data from the year 2000 and onward: ggplot(permits_se_seine, aes(Year, Value)) + geom_point() + geom_line() + scale_y_continuous(limits = c(0, max(permits_se_seine$Value))) + scale_x_continuous(limits = c(2000, max(permits_se_seine$Year))) ## Warning: Removed 18 rows containing missing values (geom_point). ## Warning: Removed 18 rows containing missing values (geom_path). Note the warning message we received: Warning message: Removed 18 rows containing missing values (geom_point). Removed 18 rows containing missing values (geom_path). That’s normal when data in your input data.frame are outside the range we’re plotting. Let’s use coord_cartesian instead to change the x and y limits: ggplot(permits_se_seine, aes(Year, Value)) + geom_point() + geom_line() + coord_cartesian(xlim = c(2000, max(permits_se_seine$Year)), ylim = c(0, max(permits_se_seine$Value))) Note the **slight* difference when using coord_cartesian: ggplot didn’t put a buffer around our values. Sometimes we want this and sometimes we don’t and it’s good to know this difference. 9.7 Scales The usual use case is to do things like changing scale limits or change the way our data are mapped onto our geom. We’ll use scales in ggplot2 very often! For example, how do we override the default colors ggplot2 uses here? ggplot(permits, aes(Value, StdDev, color = Gear)) + geom_point() Tip: Most scales follow the format `scale_{aesthetic}_{method} where aesthetic are our aesthetic mappings such as color, fill, shape and method is how the colors, fill colors, and shapes are chosen. ggplot(permits, aes(Value, StdDev, color = Gear)) + geom_point() + scale_color_manual(values = c(&quot;red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;, &quot;violet&quot;)) # ROYGBIV I’m sure that was a ton of fun to type out but we can make things easier on ourselves: ggplot(permits, aes(Value, StdDev, color = Gear)) + geom_point() + scale_color_hue(h = c(270, 360)) # blue to red Above we were using scales to scale the color aesthetic. We can also use scales to rescale our data. ggplot(permits, aes(Value, StdDev, color = Gear)) + geom_point() + scale_x_log10() Scales can also be used to change our axes. For example, we can override the labels: ggplot(permits, aes(Value, StdDev, color = Gear)) + geom_point() ## Warning: Removed 223 rows containing missing values (geom_point). permits %&gt;% group_by(Gear) %&gt;% summarize(meanval = mean(Value, na.rm = TRUE)) %&gt;% ggplot(aes(Gear, meanval)) + geom_col() + scale_x_discrete(labels = sort(unique(permits$Gear))) Or change the breaks: ggplot(permits_se_seine, aes(Year, Value)) + geom_point() + geom_line() + scale_x_continuous(breaks = c(1990, 2010)) 9.8 Facets Facets allow us to create a powerful visualization called a small multiple: http://www.latimes.com/local/lanow/la-me-g-california-drought-map-htmlstory.html I use small multiples all the time when I have a variable like a site or year and I want to quickly compare across years. Let’s create a graphical comparison of the permit prices in Cook Inlet over time: ggplot(permits_ci, aes(Year, Value)) + geom_point() + geom_line() + facet_wrap(~ Gear) facet_wrap() chose a layout for us but, in this case, it aids comparison if we stack eachpanel on top of one another: ggplot(permits_ci, aes(Year, Value)) + geom_point() + geom_line() + facet_wrap(~ Gear, ncol = 1) Note that the X and Y limits are shared across panels which is often a good default to stick with. For example, if we plot permit value over time by gear type, we don’t get a very readable plot due to differences in permit values across gear types: ggplot(permits, aes(Year, Value, group = Region)) + geom_line() + facet_wrap(~ Gear) ## Warning: Removed 2 rows containing missing values (geom_path). Note that I also added group = Region to the aes() call, which tells geom_line() to draw one line per Region instead. Allowing the Y scales to differ across panels makes things much easier to see: ggplot(permits, aes(Year, Value)) + geom_line(aes(group = Region)) + facet_wrap(~ Gear, scales = &quot;free_y&quot;) 9.9 Plot customization w/ themes ggplot2 offers us a very highly level of customizability in, what I think, is a fairly easy to discover and remember way with the theme function and pre-set themes. ggplot2 comes with a set of themes which are a quick way to get a different look to your plots. Let’s use another theme than the default: ggplot(permits, aes(Value, StdDev, color = Gear)) + geom_point() + theme_classic() Exercise: Find another theme and use it instead. Hint: Built-in themes are functions that start with theme_. The legend in ggplot2 is a thematic element. Let’s change the way the legend displays: ggplot(permits, aes(Value, StdDev, color = Gear)) + geom_point() + theme_classic() + theme(legend.position = &quot;bottom&quot;, legend.background = element_rect(fill = &quot;#EEEEEE&quot;, color = &quot;black&quot;), legend.title = element_blank(), axis.title = element_text(size = 16)) Let’s adjust our axis labels and title: ggplot(permits, aes(Value, StdDev, color = Gear)) + geom_point() + theme_classic() + theme(legend.position = c(1, 1), legend.justification = c(1,1), legend.direction = &quot;horizontal&quot;, legend.title = element_blank()) + xlab(&quot;Permit Value (unadjusted USD&quot;) + ylab(&quot;Permit Std. Dev. (unadjusted USD)&quot;) + ggtitle(&quot;Permit Standard Deviation against Permit Value by Gear Type&quot;, &quot;or: I wish I owned a seine permit&quot;) In this case, we’ve put together a theme by adding it to our plot code with a + symbol. It turns out that we can save our theme to a variable and add it to any plot we want: my_theme &lt;- theme_bw() + theme(legend.title = element_blank(), panel.grid.major = element_line(size = 1, linetype = 4), panel.grid.minor = element_blank()) ggplot(permits, aes(Value, StdDev, color = Gear)) + geom_point() + my_theme Exercise: Look at the help for ?theme and try changing something else about the above plot. More themes are available in a user-contributed package called ggthemes. 9.10 Saving plots Let’s save that great plot we just made. Saving plots in ggplot is done with the ggsave() function: ggsave(&quot;permit_stddev_vs_value.png&quot;) ggsave automatically chooses the format based on your file extension and guesses a default image size. We can customize the size with the width and height arguments: ggsave(&quot;permit_stddev_vs_value.png&quot;, width = 6, height = 6) You may notice if you often save plots that the scaling of various elements can get funky. This is because ggplot scales plot elements based on relative sizes. This most commonly manifests itself as too large/small axis labels. You can remedy this in a few ways: Change the plot dimensions Changing the base_size (theme_classic(base_size = 16)) Customize your theme (theme()) 9.11 Bonus: Multi-panel plots: Beyond facets Often times, we outgrow the small multiples (facet) paradigm. For example, we might want a line plot and a box plot to appear in the same graphic. suppressPackageStartupMessages({ library(gridExtra) }) p1 &lt;- ggplot(permits_se_seine, aes(Year, Value)) + geom_point() + geom_line() p2 &lt;- ggplot(permits %&gt;% filter(Gear == &quot;Purse Seine&quot;), aes(Region, Value)) + geom_boxplot() + scale_y_continuous(labels = function(x) { format(x, scientific = FALSE) }) grid.arrange(p1, p2) suppressPackageStartupMessages({ library(cowplot) }) plot_grid(p1, p2, align = &quot;hv&quot;, ncol = 1) ## Warning: Removed 6 rows containing non-finite values (stat_boxplot). 9.12 Bonus: Round two If we have time, we’ll walk through some ways we can augment our ggplot2 knowledge with packages that build on top of ggplot. ggplot extensions ggmap &gt; ggmap makes it easy to retrieve raster map tiles from popular online mapping services like Google Maps, OpenStreetMap, Stamen Maps, and plot them using the ggplot2 framework ggsci ggsci offers a collection of ggplot2 color palettes inspired by scientific journals, data visualization libraries, science fiction movies, and TV shows. ggbeeswarm waffle "],
["data-documentation-and-publishing.html", "10 Data Documentation and Publishing 10.1 Learning Objectives 10.2 Data sharing and preservation 10.3 Data repositories: built for data (and code) 10.4 Metadata 10.5 Structure of a data package 10.6 DataONE Federation 10.7 Publishing data from the web 10.8 Publishing data from R", " 10 Data Documentation and Publishing 10.1 Learning Objectives In this lesson, you will learn: About open data archives What science metadata is and how it can be used How data and code can be documented and published in open data archives 10.2 Data sharing and preservation 10.3 Data repositories: built for data (and code) GitHub is not an archival location Dedicated data repositories: KNB, Arctic Data Center, Zenodo, FigShare Rich metadata Archival in their mission Data papers, e.g., Scientific Data List of data repositories: http://re3data.org 10.4 Metadata Metadata are documentation describing the content, context, and structure of data to enable future interpretation and reuse of the data. Generally, metadata describe who collected the data, what data were collected, when and where it was collected, and why it was collected. For consistency, metadata are typically structured following metadata content standards such as the Ecological Metadata Language (EML). For example, here’s an excerpt of the metadata for a sockeye salmon data set: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;eml:eml packageId=&quot;df35d.442.6&quot; system=&quot;knb&quot; xmlns:eml=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;&gt; &lt;dataset&gt; &lt;title&gt;Improving Preseason Forecasts of Sockeye Salmon Runs through Salmon Smolt Monitoring in Kenai River, Alaska: 2005 - 2007&lt;/title&gt; &lt;creator id=&quot;1385594069457&quot;&gt; &lt;individualName&gt; &lt;givenName&gt;Mark&lt;/givenName&gt; &lt;surName&gt;Willette&lt;/surName&gt; &lt;/individualName&gt; &lt;organizationName&gt;Alaska Department of Fish and Game&lt;/organizationName&gt; &lt;positionName&gt;Fishery Biologist&lt;/positionName&gt; &lt;address&gt; &lt;city&gt;Soldotna&lt;/city&gt; &lt;administrativeArea&gt;Alaska&lt;/administrativeArea&gt; &lt;country&gt;USA&lt;/country&gt; &lt;/address&gt; &lt;phone phonetype=&quot;voice&quot;&gt;(907)260-2911&lt;/phone&gt; &lt;electronicMailAddress&gt;mark.willette@alaska.gov&lt;/electronicMailAddress&gt; &lt;/creator&gt; ... &lt;/dataset&gt; &lt;/eml:eml&gt; That same metadata document can be converted to HTML format and displayed in a much more readable form on the web: https://knb.ecoinformatics.org/#view/doi:10.5063/F1F18WN4 And as you can see, the whole data set or its components can be downloaded and reused. Also note that the repository tracks how many times each file has been downloaded, which gives great feedback to researchers on the activity for their published data. 10.5 Structure of a data package Note that the data set above lists a collection of files that are contained within the data set. We define a data package as a scientifically useful collection of data and metadata that a researcher wants to preserve. Sometimes a data package represents all of the data from a particular experiment, while at other times it might be all of the data from a grant, or on a topic, or associated with a paper. Whatever the extent, we define a data package as having one or more data files, softare files, and other scientific products such as graphs and images, all tied together with a descriptive metadata document. These data repositories all assign a unique identifier to every version of every data file, similarly to how it works with source code commits in GitHub. Those identifiers usually take one of two forms. A DOI identifier is often assigned to the metadata and becomes the publicly citable identifier for the package. Each of the other files gets an internal identifier, often a UUID that is globally unique. In the example above, the package can be cited with the DOI doi:10.5063/F1F18WN4. 10.6 DataONE Federation DataONE is a federation of dozens of data repositories that work together to make their systems interoperable and to provide a single unified search system that spans the repositories. DataONE aims to make it simpler for researchers to publish data to one of its member repositories, and then to discover and download that data for reuse in synthetic analyses. DataONE can be searched on the web (https://search.dataone.org/), which effectively allows a single search to find data form the dozens of members of DataONE, rather than visiting each of the currently 43 repositories one at a time. 10.7 Publishing data from the web Each data repository tends to have its own mechanism for submitting data and providing metadata. With repositories like the KNB Data Repository and the Arctic Data Center, we provide some easy to use web forms for editing and submitting a data package. Let’s walk through a web submission to see what you might expect. 10.7.1 Download the data to be used for the tutorial I’ve already uploaded the test data package, and so you can access the data here: https://test.arcticdata.io/#view/urn:uuid:630f4bcf-78f6-4c53-b495-b2f6f104202d Grab both CSV files, and the R script, and store them in a convenient folder. 10.7.2 Login via ORCID We will walk through web submission on https://test.arcticdata.io, and start by logging in with an ORCID account. ORCID provides a common account for sharing scholarly data, so if you don’t have one, you can create one when you are redirected to ORCID from the Sign In button. 10.7.3 Create and submit the data set After signing in, you can access the data submission form using the Submit button. Once on the form, drag your two csv files and R script into the package, and then provide the required metadata, which is highlighted with a red asterisk. When finished, click the Submit Dataset button at the bottom. If there are errors or missing fields, they will be highlighted. Correct those, and then try submitting again. 10.7.4 Add workflow provenance Understanding the relationships between files in a package is critically important, especially as the number of files grows. Raw data are transformed and integrated to produce derived data, that are often then used in analysis and visualization code to produce final outputs. In DataONE, we support structured descriptions of these relationships, so one can see the flow of data from raw data to derived to outputs. Here’s an example from a hydrocarbon dataset in Prince William Sound: 10.8 Publishing data from R Now lets see how to use the dataone and datapack R packages to upload data to DataONE member repositories like the KNB Data Repository and the Arctic Data Center. The dataone R package provides methods to enable R scripts to interact with DataONE to search for, download, upload and update data and metadata. The purpose of uploading data from R is to automate the repetitive tasks for large data sets with many files. For small data sets, the web submission for will certainly be simpler. The dataone R package represents the set of files in a data set as a datapack::DataPackage. We will create a DataPackage locally, and then upload it to a test version of the Arctic Data Center repository using dataone. 10.8.1 Logging in Before uploading any data to a DataONE repository, you must login to get an authentication token, which is a character string used to identify yourself. This token can be retrieved by logging into the test repository and copying the token into your R session. 10.8.2 Obtain an ORCID ORCID is a researcher identifier that provides a common way to link your researcher identity to your articles and data. An ORCID is to a researcher as a DOI is to a research article. To obtain an ORCID, register at https://orcid.org. 10.8.3 Log in to to the test repository and copy your token We will be using a test server, so login and retrieve your token at https://test.arcticdata.io Once you are logged in, navigate to your Profile Settings, and locate the “Authentication Token” section, and then copy the token for R to your clipboard. Finally, paste the token into the R Console to register it as an option for this R session. You are now logged in. But note that you need to keep this token private; don’t paste it into scripts or check it into Git, as it is just as sensitive as your password. 10.8.4 Modifying metadata Next, modify the metadata file associated with the package to set yourself as the owner. This will help us differentiate the test data later. Open the strix-pacific-northwest.xml file in RStudio, and change the givenName and surName fields at the top to your name. library(EML) source(&quot;misc/eml_helpers.R&quot;) # Load the EML file into R emlFile &lt;- &quot;data/strix/strix-pacific-northwest.xml&quot; doc &lt;- read_eml(emlFile) # Change creator to us doc@dataset@creator &lt;- c(eml_creator(&quot;Matthew&quot;, &quot;Jones&quot;, email = &quot;jones@nceas.ucsb.edu&quot;)) # Change abstract to the better one we wrote doc@dataset@abstract &lt;- as(set_TextType(&quot;data/strix/better-abstract.md&quot;), &quot;abstract&quot;) # Save it back to the filesystem write_eml(doc, emlFile) 10.8.5 Uploading A Package Using R with uploadDataPackage Datasets and metadata can be uploaded individually or as a collection. Such a collection, whether contained in local R objects or existing on a DataONE repository, will be informally referred to as a package. The steps necessary to to prepare and upload a package to DataONE using the uploadDataPackage method will be shown. A complete script that uses these steps is shown here. In the first section, we create a ’DataPackage as a container for our data and metadata and scripts. It starts out as empty. library(dataone) library(datapack) library(uuid) d1c &lt;- D1Client(&quot;STAGING&quot;, &quot;urn:node:mnTestARCTIC&quot;) dp &lt;- new(&quot;DataPackage&quot;) show(dp) We then add a metadata file, data file, R script and output data file to this package. Our first order is to generate identifiers for the files that are part of the package, and add EML metadata that references those identifiers. # Generate identifiers for our data and program objects, and add them to the metadata sourceId &lt;- paste0(&quot;urn:uuid:&quot;, uuid::UUIDgenerate()) progId &lt;- paste0(&quot;urn:uuid:&quot;, uuid::UUIDgenerate()) outputId &lt;- paste0(&quot;urn:uuid:&quot;, uuid::UUIDgenerate()) doc@dataset@otherEntity[[1]]@id &lt;- new(&quot;xml_attribute&quot;, sourceId) doc@dataset@otherEntity[[2]]@id &lt;- new(&quot;xml_attribute&quot;, progId) doc@dataset@otherEntity[[3]]@id &lt;- new(&quot;xml_attribute&quot;, outputId) repo_obj_service &lt;- paste0(d1c@mn@endpoint, &quot;/object/&quot;) doc@dataset@otherEntity[[1]]@physical[[1]]@distribution[[1]]@online@url &lt;- new(&quot;url&quot;, paste0(repo_obj_service, sourceId)) doc@dataset@otherEntity[[2]]@physical[[1]]@distribution[[1]]@online@url &lt;- new(&quot;url&quot;, paste0(repo_obj_service, progId)) doc@dataset@otherEntity[[3]]@physical[[1]]@distribution[[1]]@online@url &lt;- new(&quot;url&quot;, paste0(repo_obj_service, outputId)) write_eml(doc, emlFile) Now we have a full metadata document ready be uploaded. In the next section, we’ll add the data files and metadata to a DataPackage, and then upload that to a test repository. # Add the metadata document to the package metadataObj &lt;- new(&quot;DataObject&quot;, format=&quot;eml://ecoinformatics.org/eml-2.1.1&quot;, filename=paste(getwd(), emlFile, sep=&quot;/&quot;)) dp &lt;- addMember(dp, metadataObj) # Add our input data file to the package sourceData &lt;- &quot;data/strix/sample.csv&quot; sourceObj &lt;- new(&quot;DataObject&quot;, id = sourceId, format=&quot;text/csv&quot;, filename=paste(getwd(), sourceData, sep=&quot;/&quot;)) dp &lt;- addMember(dp, sourceObj, metadataObj) # Add our processing script to the package progFile &lt;- &quot;data/strix/filterSpecies.R&quot; progObj &lt;- new(&quot;DataObject&quot;, id = progId, format=&quot;application/R&quot;, filename=paste(getwd(), progFile, sep=&quot;/&quot;), mediaType=&quot;text/x-rsrc&quot;) dp &lt;- addMember(dp, progObj, metadataObj) # Add our derived output data file to the package outputData &lt;- &quot;data/strix/filteredSpecies.csv&quot; outputObj &lt;- new(&quot;DataObject&quot;, id = outputId, format=&quot;text/csv&quot;, filename=paste(getwd(), outputData, sep=&quot;/&quot;)) dp &lt;- addMember(dp, outputObj, metadataObj) myAccessRules &lt;- data.frame(subject=&quot;http://orcid.org/0000-0003-0077-4738&quot;, permission=&quot;changePermission&quot;) # Add the provenance relationships to the data package dp &lt;- describeWorkflow(dp, sources=sourceObj, program=progObj, derivations=outputObj) show(dp) Finally, we upload the package to the Testing server for the KNB. packageId &lt;- uploadDataPackage(d1c, dp, public=TRUE, accessRules=myAccessRules, quiet=FALSE) This particular package contains the R script filterSpecies.R, the input file sample.csv that was read by the script and the output file filteredSpecies.csv that was created by the R script, which was run at a previous time. You can now search for and view the package at https://dev.nceas.ucsb.edu: In addition, each of the uploaded entities shows the relevant provenance information, showing how the source data is linked to the derived data via the R program that was used to process the raw data: "],
["publishing-analyses-to-the-web.html", "11 Publishing Analyses to the Web 11.1 Learning Objectives 11.2 Introduction 11.3 A Minimal Example 11.4 A Less Minimal Example", " 11 Publishing Analyses to the Web 11.1 Learning Objectives In this lesson, you will learn: How to use git, GitHub (+Pages), and (R)Markdown to publish an analysis to the web 11.2 Introduction Sharing your work with others in engaging ways is an important part of the scientific process. So far in this course, we’ve introduced a small set of powerful tools for doing open science: R and its many packages RStudio git GiHub RMarkdown RMarkdown, in particular, is amazingly powerful for creating scientific reports but, so far, we haven’t tapped its full potential for sharing our work with others. In this lesson, we’re going to take an existing GitHub repository and turn it into a beautiful and easy to read web page using the tools listed above. 11.3 A Minimal Example Create a new repository on GitHub Initialize the repository on GitHub without any files in it In RStudio, Create a new Project When creating, select the option to create from Version Control -&gt; Git Enter your repository’s clone URL in the Repository URL field and fill in the rest of the details Add a new file at the top level called index.Rmd Open index.Rmd (if it isn’t already open) Press Knit Observe the renderd output Notice the new file in the same directory index.html. This is our RMarkdown file rendered as HTML (a web page) Commit your changes (to both index.Rmd and index.html) Open your web browser to the GitHub.com page for your repository Go to Settings &gt; GitHub Pages and turn on GitHub Pages for the master branch Now, the rendered website version of your repo will show up at a special URL. GitHub Pages follows a convention like this: github pages url pattern Note that it will no longer be at github.com but github.io Go to https://{username}.github.io/{repo_name}/ (Note the trailing /) Observe the awesome rendered output Now that we’ve successfully published a web page from an RMarkdown document, let’s make a change to our RMarkdown document and follow the steps to actually publish the change on the web: Go back to our index.Rmd Delete all the content, except the YAML frontmatter Type “Hello world” Commit, push Go back to https://amoeba.github.io/rmd-test/ 11.4 A Less Minimal Example Now that we’ve seen how to create a web page from RMarkdown, let’s create a website that uses some of the cool functionality available to us. We’ll use the same git repository and RStudio Project as above, but we’ll be adding some files to the repository and modifying index.Rmd. First, let’s get some data. We’ll re-use the salmon escapement data from the ADF&amp;G OceanAK database we used earlier. Navigate to https://knb.ecoinformatics.org/#view/urn:uuid:8809a404-f6e1-46a2-91c8-f094c3814b47 (or visit the KNB and search for ‘oceanak’) and copy the Download URL for the ADFG_firstAttempt_reformatted.csv file Make a folder in the top level of your repository to store the file called data Download that file into the data folder with the filename escapement_counts.csv Note that this is different than how we’ve been downloading data in earlier lessons because we’re actually going to commit the data file into git this time. Calculate median annual escapement by species using the dplyr package Display it in an interactive table with the datatable function from the DT package Make a bar plot of the median annual escapement by species using the ggplot2 package And lastly, let’s make an interactive, Google Maps-like map of the escapement sampling locations. To do this, we’ll use the leaflet package to create an interactive map with markers for all the sampling locations: First, let’s load the packages we’ll need: suppressPackageStartupMessages({ library(leaflet) library(dplyr) library(tidyr) library(ggplot2) library(DT) }) Then, let’s create the data.frame we’re going to use to plot: esc &lt;- read.csv(url(&quot;https://knb.ecoinformatics.org/knb/d1/mn/v2/object/knb.92020.1&quot;, method = &quot;libcurl&quot;), stringsAsFactors = FALSE) Now that we have the data loaded, let’s calculate median annual escapement by species: median_esc &lt;- esc %&gt;% separate(sampleDate, c(&quot;Year&quot;, &quot;Month&quot;, &quot;Day&quot;), sep = &quot;-&quot;) %&gt;% group_by(Species, Year, Location) %&gt;% summarize(escapement = sum(DailyCount)) %&gt;% group_by(Species) %&gt;% summarize(median_escapement = median(escapement)) ggplot(median_esc, aes(Species, median_escapement)) + geom_col() + coord_flip() Calculate median annual escapement by species using the dplyr package Let’s convert the escapement data into a table of just the unique locations: locations &lt;- esc %&gt;% distinct(Location, Latitude, Longitude) %&gt;% drop_na() And display it as an interactive table: datatable(locations) Then making a leaflet map is only a couple of lines of code: leaflet(locations) %&gt;% addTiles() %&gt;% addMarkers(~ Longitude, ~ Latitude, popup = ~ Location) The addTiles() function gets a base layer of tiles from OpenStreetMap which is an open alternative to Google Maps. addMarkers use a bit of an odd syntax in that it looks kind of like ggplot2 code but uses ~ before the column names. This is similar to how the lm function (and others) work but you’ll have to make sure you type the ~ for your map to work. While we can cleary see there are some serious isues with our data (note the points in Russia), this map hopefully gives you an idea of how powerful RMarkdown can be. "]
]
